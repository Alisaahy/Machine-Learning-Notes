{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bagged Tree\n",
    "\n",
    "## Bagging (Bootstrap aggregation)\n",
    "\n",
    "**Bootstrap**: To produce a bootstrap dataset, we randomly select observations from data set in order. The sampling is performed **with replacement**, which means that same observation can occur more than once in the bootstrap data set. Bootstrap is used to **quantify the uncertainty** associated with a given estimator or model. As a simple example, the bootstrap can be used to estimate the standard errors of the coefficients from a linear regression fit.\n",
    "\n",
    "\n",
    "**Bootstrap aggregation**, or **bagging**, is a general-purpose procedure for reducing the variance of a statistical learning method, frequently used in the context of decision trees.\n",
    "\n",
    "\n",
    "**Why bagging**\n",
    "\n",
    "- A single decision trees suffer from high variance. A natural way to reduce the variance is to **take many training sets from the population**, build a separate prediction model using each training set, and average the resulting predictions. For example, recall that given a set of n independent observations Z1, ..., Zn, each with variance $σ^2$, the variance of the mean $\\bar{Z}$ of the observations is given by $σ^2/n$.\n",
    "\n",
    "- Hence, bagging has been demonstrated to give impressive improvements in accuracy of a statistical learning method by combining together hundreds or even thousands of trees into a single procedure. Therefore, bagging is frequently used especially in the context of decision trees.\n",
    "\n",
    "\n",
    "**Process of Bagging**\n",
    "\n",
    "1. Take B repeated samples of size N with replacement from a dataset of size N by bootstraping\n",
    "2. We construct B trees using B bootstrapped data sets and get B predicted values\n",
    "3. For the regression setting, average all the predictions value from these trees\n",
    "\n",
    "\\begin{align}\n",
    "\\hat{f}_{bag}(x)=\\frac{1}{B}\\sum_{b=1}^B\\hat{f}^{*b}(x)\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "3. Or for the classification setting, for a given test observation, we can record the class predicted by each bootstrapped model, and take a majority vote: the overall prediction is the most commonly occurring majority class among those predictions.\n",
    "\n",
    "\n",
    "\n",
    "**number of trees B**\n",
    "\n",
    "The number of trees B is not a critical parameter with bagging: using a very large value of B will not lead to overfitting. In practice we use a value of B sufficiently large that the error has settled down. Using B = 100 is sufficient to achieve good performance in this example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes for OOB\n",
    "\n",
    "We will now derive the probability that a given observation is part of a bootstrap sample. Suppose we obtain a bootstrap sample from a set of n observations.\n",
    "\n",
    "**1. What is the probability that the first bootstrap observation is not the jth observation from the original sample? Justify your answer.**\n",
    "\n",
    "Among all n observations, every observation has a $\\frac{1}{n}$ probability to be chosen as the first bootstrap observation. So the probability that the jth observation is not the first bootstrap sample is: \n",
    "\n",
    "\\begin{align}\n",
    "1-\\frac{1}{n}\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "**2. What is the probability that the second bootstrap observation is not the jth observation from the original sample?**\n",
    "\n",
    "\\begin{align}\n",
    "1-\\frac{1}{n}\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "**3. Argue that the probability that the jth observation is not in the n bootstrap sample is $(1-\\frac{1}{n})^n$**\n",
    "\n",
    "As we are using sampling with replacement to generate the bootstrap sample, the selection probabilities are independent. So,\n",
    "\n",
    "Prob(Not 1st, Not 2nd, ..., Not the nth) = Prob(Not 1st) * Prob(Not 2nd) * … * Prob(Not nth) = $(1-\\frac{1}{n})^n$\n",
    "\n",
    "\n",
    "**4. When n = 5, what is the probability that the jth observation is in the 5 bootstrap sample?**\n",
    "\n",
    "\\begin{align}\n",
    "1-(1-\\frac{1}{n})^n = 1-(1-\\frac{1}{5})^5 = 0.67232\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "**5. When n = 100, what is the probability that the jth observation is in the 100 bootstrap sample?**\n",
    "\n",
    "\\begin{align}\n",
    "1-(1-\\frac{1}{n})^n = 1-(1-\\frac{1}{100})^{100} = 0.6339677\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "**6. When n = 10,000, what is the probability that the jth observation is in the 10,000 bootstrap sample?**\n",
    "\n",
    "\\begin{align}\n",
    "1-(1-\\frac{1}{n})^n = 1-(1-\\frac{1}{10000})^{10000} = 0.632139\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "**7. Create a plot that displays, for each integer value of n from 1 to 100,000, the probability that the jth observation is in the bootstrap sample. Comment on what you observe.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Probability that the j-th observation is in the bootstrap')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "    \n",
    "sample_sizes = range(1, 100001)\n",
    "\n",
    "probability_sequence = [(1 - (1-1/n)**n) for n in sample_sizes]\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.plot(sample_sizes, probability_sequence, color='blue')\n",
    "plt.title('Probability range')\n",
    "plt.xlabel('Sample Size')\n",
    "plt.ylabel('Probability that the j-th observation is in the bootstrap')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, the probabilities are converging to:\n",
    "\n",
    "\\begin{align}\n",
    "\\lim_{n \\to \\infty} [1 - (1-\\frac{1}{n})^n]\n",
    "\\end{align}\n",
    "\n",
    "Recall that:\n",
    "\n",
    "\\begin{align}\n",
    "\\lim_{n \\to \\infty} (1+\\frac{x}{n})^n = e^x\n",
    "\\end{align}\n",
    "\n",
    "So,\n",
    "\n",
    "\\begin{align}\n",
    "\\lim_{n \\to \\infty} [1 - (1-\\frac{1}{n})^n] = \\lim_{n \\to \\infty} [1 - (1+\\frac{(-1)}{n})^n] = 1-e^{-1} = 0.632 \\approx \\frac{2}{3}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Out-of-Bag Error Estimation\n",
    "\n",
    "**A very straightforward way to estimate the test error of a bagged model, without cross-validation or the validation set**\n",
    "\n",
    "- As we calculated before, if each time we draw N samples with replacement from a dataset of size N, approximately 2/3 of the dataset will be included in the bootstrapped dataset, which means that every bagged tree **makes use of around 2/3 the observations**. The remaining 1/3 of the observations not used to fit a given bagged tree are referred to as the **out-of-bag (OOB)** observations. We can **predict the response for those OOB observations** using each of the trees. \n",
    "\n",
    "\n",
    "- If we the number of trees we built is B. **For every observation, this will yield around B/3 predictions**. In order to obtain a single prediction for the ith observation, if regression, we can **average** these predicted responsesor can take a **majority** vote if classification is the goal.\n",
    "\n",
    "\n",
    "- An OOB prediction can be obtained in this way for **each of the N observations**, from which the overall OOB MSE (for a regression problem) or classification error (for a classification problem) can be computed. The resulting OOB error is a valid estimate of the test error for the bagged model, since the response for each observation is predicted using only the trees that were not fit using that observation.\n",
    "\n",
    "\n",
    "- The OOB approach for estimating the test error is particularly convenient when performing **bagging** on large data sets for which cross-validation would be computationally onerous."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variable Importance Measures \n",
    "\n",
    "**Bagging improves prediction accuracy at the expense of interpretability**\n",
    "\n",
    "- When we bag a large number of trees, it is no longer possible to **represent the resulting statistical learning procedure using a single tree**\n",
    "\n",
    "\n",
    "**Variable Importance**\n",
    "\n",
    "- One can obtain an overall summary of the importance of each predictor using the RSS (for bagging regression trees) or the Gini index (for bagging classification trees).\n",
    "\n",
    "- **For bagging regression trees**: Record the total amount that the RSS is decreased due to splits over a given predictor, averaged over all B trees. A large value indicates an important predictor. \n",
    "\n",
    "\\begin{align}\n",
    "RSS=\\sum_{j=1}^J\\sum_{i \\in R_j} (y_i-\\hat{y}_{R_j})^2\n",
    "\\end{align}\n",
    "\n",
    "- **For bagging classification trees**: Add up the total amount that the **Gini index** is decreased\n",
    "by splits over a given predictor, averaged over all B trees.\n",
    "\n",
    "<img src=\"./images/92.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forests\n",
    "\n",
    "Random forests provide an **improvement** over bagged trees by way of a small tweak that **decorrelates** the trees.\n",
    "\n",
    "\n",
    "**Bagged trees**\n",
    "\n",
    "- Suppose that there is one **very strong** predictor in the data set, along with a num- ber of other moderately strong predictors. Then in the collection of bagged trees, most or all of the trees will **use this strong predictor in the top split**.\n",
    "- Consequently, all of the bagged trees will **look quite similar** to each other. Hence the predictions from the bagged trees will be **highly correlated**.\n",
    "- Unfortunately, averaging many highly correlated quantities does not lead to as large of a reduction in variance as averaging many uncorrelated quantities. This means that bagging will **not lead to a substantial reduction in variance** over a single tree in this setting.\n",
    "\n",
    "\n",
    "**Random forests**\n",
    "\n",
    "Random forests overcome this problem by forcing each split to consider only a subset of the predictors.\n",
    "\n",
    "- Each time a split in a tree is considered, **a random sample of m predictors** is chosen as split candidates from the full set of p predictors. The split is allowed to use only one of those **m** predictors. **A fresh sample** of m predictors is taken at each split, and typically we choose $m \\approx \\sqrt{p}$\n",
    "- Therefore, on average (p − m)/p of the splits will not even consider the strong predictor, and so other predictors will have more of a chance. \n",
    "- We can think of this process as **decorrelating the trees**, thereby making the average of the **resulting trees less variable** and hence more reliable.\n",
    "\n",
    "\n",
    "**Main difference**\n",
    "\n",
    "The main difference between bagging and random forests is **the choice of predictor subset size m**. \n",
    "- If a random forest is built using m = p, then this amounts simply to bagging. \n",
    "-  Using a small value of m in building a random forest will typically be helpful when we have **a large number of correlated predictors (high-dimensional data)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boosting\n",
    "\n",
    "**Boosting**: approach for improving the predictions resulting from a decision tree. Like bagging, boosting is a **general approach** that can be applied to many statistical learning methods for regression or classification. Here we talk about boosting in decision trees setting. \n",
    "\n",
    "\n",
    "**Bagging**\n",
    "\n",
    "Each tree is built on a bootstrap data set, independent of the other trees.\n",
    "\n",
    "\n",
    "\n",
    "**Boosting**\n",
    "\n",
    "- Trees are grown **sequentially**: each tree is grown using information from previously grown trees. \n",
    "- Boosting does not involve bootstrap sampling; instead each tree is fit on a modified version of the original data set.\n",
    "\n",
    "\n",
    "**Algorithm of Boosting for Regression Trees**\n",
    "\n",
    "1. Set $\\hat{f(x)}=0$ and $r_i = y_i$ for all i in the training set (predict 0 for all training observations and hence residual values euqal to their original response values)\n",
    "2. For b=1, 2, ..., B (every dicision tree we build), repeat:\n",
    " - Fit a tree $\\hat{fˆb}$ with d splits (d+1 terminal nodes) to the training data (X, r). That is, we fit a tree using the current residuals, rather than the outcome Y , as the response. \n",
    " - Update $\\hat{f}$ by adding in a shrunken version of the new tree. That is, we add this new decision tree into the fitted function in order to update the residuals.\n",
    "\\begin{align}\n",
    "\\hat{f}(x) \\leftarrow \\hat{f}(x) + \\lambda\\hat{f}^b(x)\n",
    "\\end{align}\n",
    " - Update the residuals\n",
    "\\begin{align}\n",
    "r_i \\leftarrow r_i \\leftarrow \\lambda\\hat{f}^b(x_i)\n",
    "\\end{align}\n",
    "3. Output the boosted model\n",
    "\\begin{align}\n",
    "\\hat{f}(x) = \\sum_{b=1}^B\\lambda\\hat{f}^b(x)\n",
    "\\end{align}\n",
    "\n",
    "By fitting small trees to the residuals, we slowly **improve f in areas where it does not perform well**. The shrinkage parameter λ slows the process down even further, allowing more and different shaped trees to attack the residuals.\n",
    "\n",
    "\n",
    "**Boosting has three tuning parameters:**\n",
    "\n",
    "1. The number of trees B. Unlike bagging and random forests, **boosting can overfit if B is too large**, although this overfitting tends to occur slowly if at all. We use cross-validation to select B.\n",
    "\n",
    "2. The shrinkage parameter λ, a small positive number. This controls **the rate at which boosting learns**. Typical values are 0.01 or 0.001, and the right choice can depend on the problem. Very small λ can require using a very large value of B in order to achieve good performance.\n",
    "\n",
    "3. The number d of splits in each tree, which controls the **complexity** of the boosted ensemble. Often d = 1 works well, in which case each tree is a stump, consisting of a single split. In this case, the boosted ensemble is fitting an additive model, since each term involves only a single variable. More generally d is the **interaction depth**, and controls the interaction order of the boosted model, since d splits can **involve at most d variables**.\n",
    "\n",
    "\n",
    "**Boosting V.S. Random forests:**\n",
    "\n",
    "- In boosting, because the growth of a particular tree **takes into account the other trees** that have already been grown, **smaller trees are typically sufficient**. \n",
    "- Using smaller trees can aid in **interpretability** as well; for instance, using **stumps** leads to an additive model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "273px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
