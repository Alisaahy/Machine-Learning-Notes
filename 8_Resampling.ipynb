{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Resampling:** Resampling methods involve **repeatedly drawing samples** from a training set and **refitting** a model of interest on each sample in order to obtain additional information about the fitted model. \n",
    "\n",
    "> For example, in order to estimate the variability of a linear regression fit, we can repeatedly draw different samples from the training data, fit a linear regression to each new sample, and then examine the extent to which the resulting fits differ. \n",
    "\n",
    "**Model assessment**: The process of evaluating a model’s performance\n",
    "\n",
    "**Model selection**: The process of selecting the proper level of flexibility for a model\n",
    "\n",
    "**bootstrap**:provide a measure of accuracy of a parameter estimate or of a given selection statistical learning method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-Validation\n",
    "\n",
    "**Cross-validation:** in the absence of a very large designated test set that can be used to directly estimate the test error rate, a number of techniques can be used to estimate this quantity using the available training data. **Cross-validation** estimates the test error rate by **holding out a subset of the training observations** from the fitting process, and then applying the statistical learning method to those held out observations.\n",
    "\n",
    "\n",
    "## The Validation Set Approach\n",
    "\n",
    "1. Randomly dividing the available set of observations into two parts, a **training set** and a **validation set** or hold-out set. \n",
    "\n",
    "2. The model is fit on the training set, and the fitted model is used to predict the responses for the observations in the validation set. \n",
    "\n",
    "3. The resulting validation set error rate provides an estimate of the test error rate.\n",
    "\n",
    "**Drawbacks**:\n",
    "\n",
    "1. The validation estimate of the test error rate can be highly **variable**, depending on precisely which observations are included in the training set and which observations are included in the validation set.\n",
    "\n",
    "2. In the validation approach, only **a subset of the observations**—those that are included in the training set rather than in the validation set **are used to fit the model**. Since statistical methods tend to perform worse when trained on fewer observations, this suggests that the validation set error rate may tend to **overestimate the test error rate for the model fit on the entire data set**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leave-One-Out Cross-Validation\n",
    "\n",
    "1. Splitting the set of observations into two parts, **a single observation (x1,y1)** is used for the validation set, and the remaining observations {(x2, y2), . . . , (xn, yn)} make up the training set. \n",
    "\n",
    "2. The model is fit on the n − 1 training observations, and a prediction is made for the excluded observation, using its value x1. Calculate MSE for the first validation pair, which is **$MSE_1 = (y_1−\\hat{y_1})^2$**. Since (x1, y1) was not used in the fitting process, $MSE_1$ provides an approximately **unbiased estimate for the test error**.\n",
    "\n",
    "3. Repeat the procedure by selecting (x2,y2) for the validation data, training the statistical learning procedure on the remaining n − 1 observations, and computing $MSE_2 = (y_2−\\hat{y_2})^2$. Repeatng this approach n times produces n squared errors, $MSE_1, MSE_2, MSE_3, ..., MSE_n$. The LOOCV estimate for the test MSE is the average of these n test error estimates:\n",
    "\n",
    "\\begin{align}\n",
    "CV_{(k)}=\\frac{1}{k}\\sum_{i=1}^kMSE_i\n",
    "\\end{align}\n",
    "\n",
    "**Advantages over validation set approach:**\n",
    "1. It has far less bias. In LOOCV, we repeatedly fit the model using training sets that contain n − 1 observations, almost as many as are in the entire data set. Consequently, the LOOCV approach tends **not to overestimate the test error rate** as much as the validation set approach does.\n",
    "2. In contrast to the validation approach which will yield different results when applied repeatedly due to randomness in the training/validation set splits, performing LOOCV multiple times will always yield **the same results** because there is **no randomness in the training/validation set splits**.\n",
    "\n",
    "**Drawbacks:**\n",
    "\n",
    "LOOCV has the potential to be expensive to implement, since the model has to be **fit n times**. This can be very **time consuming** if n is large, and if each individual model is slow to fit. \n",
    "> Exceptions: With least squares linear or polynomial regression, an amazing shortcut makes the cost of LOOCV the same as that of a single model fit. We can only fit the model once and then use the diagonal elements to calculate LOOCV."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Fold Cross-Validation\n",
    "\n",
    "1. Randomly k-fold CV dividing the set of observations into k groups, or folds, of approximately equal size. \n",
    "2. The first fold is treated as a validation set, and the method is fit on the remaining k − 1 folds. \n",
    "3. The mean squared error, $MSE_1$, is then computed on the observations in the held-out fold. This procedure is repeated k times; each time, a different group of observations is treated as a validation set. \n",
    "4. This process results in k estimates of the test error, $MSE_1, MSE_2, ..., MSE_k$. \n",
    "5. The k-fold CV estimate is computed by averaging these values,\n",
    "\n",
    "\\begin{align}\n",
    "CV_{(k)}=\\frac{1}{k}\\sum_{i=1}^kMSE_i\n",
    "\\end{align}\n",
    "\n",
    "**Advantages over LOOCV approach:**\n",
    "\n",
    "1. Computational advantage: LOOCV requires fitting the model n times. This has the potential to be **computationally expensive** (except for linear models fit by least squares), especially that some models have computationally intensive fitting procedures. So performing LOOCV may pose computational problems, **especially if n is extremely large**. And k-fold CV will be much more feasible and flexible.\n",
    "2. K-fold CV often gives more accurate estimates of the test error rate than does LOOCV. This has to do with a bias-variance trade-off.\n",
    "\n",
    "**Goal:**\n",
    "\n",
    "1. Determine how well a model can be expected to perform on independent data\n",
    "2. Interested in the model which yield the minimum MSE curve. We might be performing cross-validation on a number of model, or on a single model using different levels of flexibility, in order to identify the method that results in the lowest test error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias-Variance Trade-Off for k-Fold Cross-Validation\n",
    "\n",
    "- K-fold CV is more biased than LOOCV and K-fold CV often gives more accurate estimates of the test error rate\n",
    " - LOOCV will give approximately unbiased estimates of the test error, since each training set contains n − 1 observations, which is almost as many as the number of observations in the full data set.\n",
    " - k-fold CV for, say, k = 5 or k = 10 will lead to an intermediate level of bias\n",
    "\n",
    "- K-Fold with k < n will have less variance than LOOCV\n",
    " - When we perform LOOCV, we are in effect averaging the outputs of n fitted models, each of which is trained on an almost identical set of observations; therefore, these outputs are highly (positively) correlated with each other. \n",
    " - When we perform k-fold CV with k < n, we are averaging the outputs of k fitted models that are somewhat less correlated with each other, since the overlap between the training sets in each model is smaller. \n",
    " - The mean of many highly correlated quantities has higher variance than does the mean of many quantities that are not as highly correlated, the test error estimate resulting from LOOCV tends to have higher variance than does the test error estimate resulting from k-fold CV."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Bootstrap\n",
    "\n",
    "Bootstrap is used to **quantify the uncertainty** associated with a given estimator or model. As a simple example, the bootstrap can be used to estimate the standard errors of the coefficients from a linear regression fit. \n",
    "\n",
    "1. A data set, which we call Z, that contains n observations. We randomly select n observations from the data set in order to produce a bootstrap data set, $Z^{∗1}$. \n",
    "\n",
    "2. The sampling is performed with **replacement**, which means that same observation can occur more than once in the bootstrap data set. \n",
    " - In this example, $Z^{∗1}$ contains the third observation twice, the first observation once, and no instances of the second observation. \n",
    " - Note that if an observation is contained in $Z^{∗1}$, then both its X and Y values are included. \n",
    " \n",
    "3. We can use $Z^{∗1}$ to produce a new bootstrap estimate for α (a value we can calculate from each bootstrap), which we call $\\alpha^{∗1}$. This procedure is repeated B times for some large value of B, in order to produce B different bootstrap data sets, $Z^{∗1}$,$Z^{∗2}$, . . . , $Z^{∗B}$, and B corresponding α estimates, $\\alpha^{∗1}$, $\\alpha^{∗2}$, . . . , $\\alpha^{∗B}$. \n",
    "\n",
    "4. We can compute the standard error of these bootstrap estimates using the formula\n",
    "\\begin{align}\n",
    "SE_B(\\hat{\\alpha})=\\sqrt{\\frac{1}{B-1}\\sum_{i=1}^B\\left( \\hat{\\alpha}^{*i}-\\frac{1}{B}\\sum^{B}_{j=1}\\hat{\\alpha}^{*j} \\right)}\n",
    "\\end{align}\n",
    "This serves as an estimate of the standard error of $\\hat{\\alpha}$ estimated from the original data set.\n",
    "\n",
    "<img src=\"./images/64.png\" width=600>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
