{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# High-Dimensional Data\n",
    "\n",
    "**High-dimensional**: Data sets containing more features than observations are often referred to as high-dimensional. (p > n)\n",
    "\n",
    "**Example**\n",
    "\n",
    "1. To predict blood pressure, one might also collect measurements for half a million low-dimensional single nucleotide polymorphisms (SNPs; these are individual DNA mutations that are relatively common in the population). n ≈ 200 and p ≈ 500,000.\n",
    "\n",
    "2. To understand people’s online shopping patterns. We treat all of the search terms entered by users of a search engine as features. This is sometimes known as the “bag-of- words” model. For a given user, each of the p search terms is scored present (0) or absent (1), **creating a large binary feature vector**. Then n ≈ 1,000 and p is much larger."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why Considering High Dimensional Data(also applies if p is slightly smaller than n)\n",
    "\n",
    "\n",
    "1. When the number of features p is as large as, or > n, least squares cannot be performed. Regardless of whether or not there truly is a relationship between the features and the response, least squares will yield a set of coefficient estimates that result in a **perfect fit** to the data, such that the **residuals are zero**, which certainly leads to **overfitting of the data**.\n",
    "\n",
    "The problem is simple: when p > n or p ≈ n, a simple least squares regression line is too ***flexible*** and hence overfits the data.\n",
    " \n",
    "<img src=\"./images/74.png\" width=650>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "2. Examines only the R2 or the training set MSE might erroneously conclude that the model with the greatest number of variables is best.\n",
    "<img src=\"./images/75.png\" width=650>\n",
    "\n",
    " - **Cp, AIC, and BIC** approaches are not appropriate in the high-dimensional setting, because estimating ˆσ2 is problematic.(For instance, the formula for ˆσ2 from Chapter 3 yields an estimate ˆσ2 = 0 in this setting.) \n",
    " \n",
    " - **Adjusted R2** in the high-dimensional setting is problematic, since one can easily obtain a model with an adjusted R2 value of 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression in High Dimensions\n",
    "\n",
    "\n",
    "**Three important points:**\n",
    "(1) regularization, subset selection or dimension reduction methods play a key role in high-dimensional problems, which avoid overfitting by using a less flexible fitting approach\n",
    "\n",
    "- feature selection (forward stepwise selection / backward stepwise selection)\n",
    "- ridge regression\n",
    "- the lasso\n",
    "- PCA/PCR\n",
    "\n",
    "(2) deciding the number of predictor included is crucial for good predictive performance\n",
    "\n",
    "(3) the test error tends to increase as the dimensionality increases\n",
    "\n",
    " - Noise features increase the dimensionality of the problem, **increasing the risk of overfitting**. Noise features may have associations with the response on the training set by chance but cannot get generalized to the test dataset.\n",
    " - Even additional features are relevant, the increased variance may outweigh the reduction in bias that they bring, and caused increased error.\n",
    "\n",
    "\n",
    "## Curse of dimensionality\n",
    "- Adding additional signal features that are **truly associated** with the response will **improve** the fitted model; \n",
    "\n",
    "- However, adding noise features that are **not truly associated with the response** will lead to a **deterioration** in the fitted model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpreting Results in High Dimensions\n",
    "\n",
    "1. In the high-dimensional setting, the **multicollinearity** problem is extreme: any variable in the model can be written as a linear combination of all of the other variables in the model. This means that we can hardly know exactly which variables truly are predictive of the outcome.\n",
    "\n",
    "> For example, if we use forward stepwise selection to extract 20 features from the original dataset with a million variables and use these selected features to build a predictive model on the training data. However, there are likely to be many sets of 20 features that would be equally predictive as well as the selected model. So we should make it clear that what we have identified is simply **one of many possible models** for predicting the response, and it must be further validated on test data sets.\n",
    "\n",
    "2. Be cautious in reporting errors and measures of model fit in the high-dimensional setting\n",
    " - **Therefore, one should never use sum of squared errors, p-values, R2 statistics, or other traditional measures of model fit on the training data as evidence of a good model fit in the high-dimensional setting**. For example, when p > n, it is easy to obtain a useless model that has zero residuals OR a model with R2 = 1\n",
    " \n",
    " - It is important to instead report results on an independent **test set**, or **cross-validation errors**. For instance, the MSE or R2 on an independent test set is a valid measure of model fit, but the MSE on the training set certainly is not."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
