{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple linear regression\n",
    "\n",
    "Simple linear regression predicts a quantitative response Y on the basis of a single predictor variable X. \n",
    "\n",
    "It assumes that there is approximately a linear relationship between X and Y:\n",
    "\n",
    "\\begin{align}\n",
    "Y \\approx \\beta_0+\\beta_1X\n",
    "\\end{align}\n",
    "\n",
    "${β_0}$ and ${β_1}$ are **intercept** and **slope** terms in linear model.\n",
    "The prediction for Y on the basis of X = $x$:\n",
    "\n",
    "\\begin{align}\n",
    "\\hat{y} = \\hat{β_0} + \\hat{β_1}x\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimating the coefficients\n",
    "\n",
    "By far, the most common approach for estimating coefficients is minimizing the $least$ $squares$ criterion.\n",
    "\n",
    "$\\hat{y_i} = \\hat{β_0} + \\hat{β_1}x_i$ be the prediction for Y based on the $i$th value of X,\n",
    "\n",
    "$e_i = y_i−\\hat{y_i}$ represents the ith residual—this is the difference between the ith observed value and the ith predicted value.\n",
    "\n",
    "We define the **residual sum of squares (RSS)** as\n",
    "\n",
    "\\begin{align}\n",
    "RSS&=e_1^2+e_2^2+e_3^2+...e_n^2 \\\\\n",
    "RSS&=(y_1−\\hat{β_0} - \\hat{β_1}x_1)^2+(y_2−\\hat{β_0} - \\hat{β_1}x_2)^2,..., (y_n−\\hat{β_0} - \\hat{β_1}x_n)^2\n",
    "\\end{align}\n",
    "\n",
    "The least squares approach chooses $\\hat{β_1}$ and $\\hat{β_0}$ to minimize the **RSS**:\n",
    "\n",
    "\\begin{align}\n",
    "\\hat{\\beta_1}&=\\frac{\\sum_{i=1}^n(x_i-\\bar{x})(y_i-\\bar{y})}{\\sum_{i=1}^n(x_i-\\bar{x})^2} \\\\\n",
    "\\hat{\\beta_0}&=\\bar{y}-\\hat{\\beta_1}\\bar{x}  \\quad \\quad\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "where $\\bar{y}=\\sum_{i=1}^ny_i/n$, $\\bar{x}=\\sum_{i=1}^nx_i/n$ are the **sample means**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assessing the accuracy of the coefficient estimates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## True regression line (Population regression line) V.S. Least square regression line\n",
    "- True regression line:\n",
    "\n",
    "Assume that the **true relationship** between $X$ and $Y$ takes the form $Y = f(X) + \\epsilon$ for some unknown function $f$, where $\\epsilon$ is a mean-zero random error term. \n",
    "\n",
    "If $f$ is to be approximated by a linear function, then the relationship is: \n",
    "\n",
    "\\begin{align}\n",
    "Y=\\beta_0+\\beta_1X+\\epsilon\n",
    "\\end{align}\n",
    "\n",
    "This is also the **population regression line**, where the error term is a catch-all for what we missed with this simple model: other variables that cause variation in $Y$, or measurement error. We assume the error is independet of $X$. We assume that the error term is independent of $X$.\n",
    "\n",
    "\n",
    "- Least squares line:\n",
    "\n",
    "The **least squares line** is the what we can compute from a set of sample observations. We use information from this sample to estimate coefficients of the population regression line ${β_1}$ and ${β_0}$:\n",
    "\n",
    "\\begin{align}\n",
    "\\hat{y_i} = \\hat{β_0} + \\hat{β_1}x_i\n",
    "\\end{align}\n",
    "\n",
    "<img src=\"images/5.png\" width=500>\n",
    "\n",
    "Notice that different sample datasets (generated from the same true model) result in slightly different least squares lines, but the unobserved population line doesn't change.\n",
    "\n",
    "**Unbiased**\n",
    "- If we can average the estimated ${β_1}$ and ${β_0}$ obtained over a huge number of datasets, then the average of them would be unbiased, in the sense that on average, we expect estimations to equal true values.\n",
    "- An unbiased estimator does not systematically over- or under-estimate the true parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How accurate is the $\\hat{\\beta_0}$ and $\\hat{\\beta_1}$ as estimates of true $\\beta_0$ and $\\beta_1$?\n",
    "\n",
    "**In general, we answer this question by computing the $standard$ $error$ of coefficients.**\n",
    "\n",
    "\n",
    "### Step 1: calculate standard error of coefficients with estimated variance\n",
    "\n",
    "- **Standard Deviation V.S. Standard Error**\n",
    "\n",
    "Standard deviation (SD) measures the amount of variability, or dispersion, for a subject set of data from the mean.\n",
    "\n",
    "Standard error of the mean (SEM) measures how far the sample mean of the data is likely to be from the true population mean. \n",
    "\n",
    "<img src=\"images/6.png\" width=350>\n",
    "\n",
    "\n",
    "- **Calculate standard error of coefficients:**\n",
    "\n",
    "\\begin{align}\n",
    "Intercept: SE(\\hat{\\beta_0})^2&=\\sigma^2\\left[\\frac{1}{n}+\\frac{\\bar{x}^2}{\\sum_{i=1}^n(x_i-\\bar{x})^2}  \\right]  \\\\\n",
    "Slope: SE(\\hat{\\beta_1})^2&=\\frac{\\sigma^2}{\\sum_{i=1}^n(x_i-\\bar{x})^2} \n",
    "\\end{align}\n",
    "\n",
    "where $\\sigma^2 = Var(\\epsilon)$. \n",
    "For these formulas to be strictly valid, we need to assume that \n",
    "the errors $\\epsilon_i$ in the population regression line for each observation, are uncorrelated with common variance $\\sigma^2$. \n",
    "\n",
    "** Notice that the $SE(\\hat{\\beta_1})$ us smaller when the ${x_i}$ is more spread out; intuitively we have more leverage to estimate a slope.\n",
    "\n",
    "\n",
    "- **Estimate $\\sigma^2$**\n",
    "\n",
    "$\\sigma$ is not known, but can be estimated from the data. The estimate of $\\sigma$ is known as the **RSE (residual standard error)**:\n",
    "\n",
    "\\begin{align}\n",
    "RSE=\\sqrt{RSS/(n-2)}\n",
    "\\end{align}\n",
    "\n",
    "> ** The denominator n-2 is degree of freedom. The reason n-2 is that we estimate RSE from the RSS of the least squares line (RSS = sum of squared residuals) as a proxy of the $Var(\\epsilon)$ (variance of the $\\epsilon$ term. In the population regression line, and in this process, 2 parameters (the slope and the intercept) were firstly estimated.\n",
    "\n",
    "> ** A residual is the difference between the observed value and the predicted value (by the model, which is the least squares line). An error is the difference between the observed value and the true value (very often unobserved, in the true population line).\n",
    "\n",
    "### Step 2: calculate confidence Intervals of coefficients\n",
    "\n",
    "**A 95% confidence interval**: is defined as \n",
    "a range of values such that with 95% probability, the range will contain the true unknown value of the parameter.\n",
    "\n",
    "For linear regression, the 95% confidence interval for $β1$ approximately takes the form:\n",
    "\n",
    "\\begin{align}\n",
    "\\hat{\\beta_1} \\pm 2 \\cdot SE(\\hat{\\beta_1})     \n",
    "\\end{align}\n",
    "\n",
    "> ** The equation relies on the assumption that the errors are Gaussian. A\n",
    "lso, the factor of 2 in front of the $SE(\\hat{\\beta_1})$ term will vary slightly depending on the number of observations n in the linear regression. \n",
    "To be precise, rather than the number 2, it should contain the 97.5% quantile of a t-distribution with n−2 degrees of freedom.\n",
    "\n",
    "\n",
    "### Step 3: make hypothesis testing of coefficients\n",
    "\n",
    "Testing the **null test hypothesis** of\n",
    "\n",
    "```\n",
    "H_0: There is no relationship between X and Y, or \n",
    "H_0: β1=0\n",
    "```\n",
    "\n",
    "versus the **alternative hypothesis**\n",
    "\n",
    "```\n",
    "H_a: There is some relationship between X and Y, or \n",
    "H_a: β1≠0\n",
    "```\n",
    "\n",
    "To test the hypothesis, we want to see  \n",
    "- whether $\\hat{\\beta_1}$ is sufficiently from 0,\n",
    "- and if $SE(\\hat{\\beta_1})$ is small, then even relatively small values of $\\hat{\\beta_1}$ can provide strong evidence that $\\beta_1 \\neq 0$, and hence that there is a relationship between X and Y.\n",
    "\n",
    "\n",
    "In practice, we compute a **t-statistic**, given by\n",
    "\\begin{align}\n",
    "t=\\frac{\\hat{\\beta_1}-0}{SE(\\hat{\\beta_1})} \n",
    "\\end{align}\n",
    "> which measures the number of standard deviations that $\\hat{\\beta_1}$ is away from 0.If there really is no relationship between X and Y , then we expect it will have a t-distribution with n−2 degrees of freedom.\n",
    "\n",
    "The t-distribution has a bell shape and for sample sizes > 30, it's quite similar to the normal distribution. And we can compute the **p-value** corresponding to this t-statistic, which is the probability of observing any number equal to |t| or larger in absolute value, assuming $\\hat{\\beta_1} = 0$.\n",
    "\n",
    "**Interpretation:**\n",
    "- A small p-value indicates that it's unlikely to observe such a substantial association between the predictor and the target due to chance, in the absence of any real association. \n",
    "- And we can infer that there's an association between the predictor and the target. \n",
    "- We can **reject the null hypothesis**.\n",
    "\n",
    "\n",
    "**Example:**\n",
    "<img src=\"images/7.png\" width=400>\n",
    "\n",
    "Here |t|=2.17, p-value=0.015. The area in red is 0.015 + 0.015 = 0.030, 3%. If we had chosen a significance level of 5%, this would mean that we had achieved statistical significance. We can reject the null hypothesis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Assessing the Accuracy of the Model\n",
    "\n",
    "Once we have rejected the null hypothesis, we will further qualify **the extent to which the model fits the data**.\n",
    "The quality of a linear regression fit is typically assessed using two related quantities: **the residual standard error (RSE)** and the **R2** statistic.\n",
    "\n",
    "## RSE (Residual Standard Error)\n",
    "The RSE is an estimate of the standard deviation of $\\epsilon$. Roughly speaking, it is the average amount that the response will deviate from the true regression line.\n",
    "\n",
    "\\begin{align}\n",
    "RSS&=\\sum_{i=1}^n(y_i-\\hat{y})^2  \\\\\n",
    "RSE&=\\sqrt{\\frac{1}{n-2}RSS}=\\sqrt{\\frac{1}{n-2}\\sum_{i=1}^n(y_i-\\hat{y})^2}\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "<img src=\"images/8.png\" width=600>\n",
    "\n",
    "In the case of the advertising data, we see from the linear regression output in Table 3.2 that the RSE is 3.26. In other words, actual sales in each market deviate from the true regression line by approximately 3,260 units, on average (the target variable is in thousands of units). \n",
    "\n",
    "In other words, even if the model were correct and the true values of the unknown coefficients ${β_0}$ and ${β_1}$ were known exactly, any prediction of sales on the basis of TV advertising would still be off by about 3.260 units on average.\n",
    "\n",
    "The mean value of sales over all markets is approximately 14,000 units, and so the percentage error is 3,260/14,000 = 23%.\n",
    "\n",
    "> The RSE is considered a measure of the **lack of fit** of the model $Y = β_0 + β_1X + \\epsilon$ to the data. If the predictions obtained using the model are very close to the true outcome values, then RSE will be small, and we can conclude that the model fits the data very well.\n",
    "\n",
    "## R2 Statistic\n",
    "\n",
    "he RSE is measured in the units of Y, it is not always clear what constitutes a good RSE. \n",
    "\n",
    "The $R^2$ statistic has an interpretational advantage over RSE. It takes the form of a proportion—**the proportion of variance explained**—and so it always takes on a value between 0 and 1, and is independent of the scale of Y .\n",
    "\n",
    "\\begin{align}\n",
    "R^2 = \\frac{(TSS − RSS)}{TSS}= 1− \\frac{RSS}{TSS}\n",
    "\\end{align}\n",
    "\n",
    "**TSS(total sum of squares)**: $\\sum_{i=1}^n(y_i-\\bar{y})^2$ - the total amount of variability inherently in the target variabe before the regression is performed\n",
    "\n",
    "**RSS**: $\\sum_{i=1}^n(y_i-\\hat{y})^2$ - the amount of variability that is left unexplained after performing the regression\n",
    "\n",
    "**TSS−RSS**: measures the amount of variability in the target variable that is explained (or removed) by performing the regression, and $R^2$ measures the proportion of variability in Y that can be explained using X.\n",
    "\n",
    "**Interpretation**：\n",
    "- close to 1 : a large proportion of the variability in the target has been explained by the regression. \n",
    "- close to 0 : the regression did not explain much of the variability in the target\n",
    " - the linear model is wrong\n",
    " - the inherent error $σ^2$ is high, or both.\n",
    "\n",
    "\n",
    "> Squared Correlation V.S. R2 Statistic\n",
    "\n",
    ">**Correlation**:\n",
    "\n",
    ">\\begin{align}\n",
    "Cor(X,Y)=\\frac{\\sum_{i=1}^n(x_i-\\bar{x})(y_i-\\bar{y})}{\\sqrt{\\sum_{i=1}^n(x_i-\\bar{x})^2}\\sqrt{\\sum_{i=1}^n(y_i-\\bar{y})^2}}\n",
    "\\end{align}\n",
    "\n",
    ">is also a measure of the linear relationship between X and Y.\n",
    "\n",
    "> In the **simple linear regression** setting, $R^2 = [Cor]^2$. In other words, the squared correlation and the R2 statistic are identical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple Linear Regression\n",
    "\n",
    "multiple\n",
    "linear regression model takes the form:\n",
    "\n",
    "\\begin{align}\n",
    "Y=\\beta_0+\\beta_1X_1+,,,+\\beta_pX_p+\\epsilon\n",
    "\\end{align}\n",
    "\n",
    "## Estimating the Regression Coefficients \n",
    "\n",
    "The parameters are estimated using the same least squares approach that we choose β0, β1, . . . , βp to minimize the sum of squared residuals\n",
    "\n",
    "\\begin{align}\n",
    "RSS&=\\sum_{i=1}^n(y_i-\\hat{y}_i)^2 \\\\\n",
    "&=\\sum_{i=1}^n(y_i-\\hat{\\beta_0}-\\hat{\\beta_1}x_{i1}-,,,-\\hat{\\beta_p}x_{ip})^2\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "<img src=\"images/9.png\" width=600>\n",
    "Table 3.4 displays the multiple regression coefficient estimates. The coefficient for newspaper in the model is close to zero and the corresponding p-value is no longer siginificant. \n",
    "**Does it make sense for the multiple regression to suggest no relationship between *sales* and *newspaper* while the simple linear regression implies the opposite?**\n",
    "\n",
    "- Notice that the correlation between radio and newspaper is 0.35. This reveals a tendency to spend more on newspaper advertising in markets where more is spent on radio advertising. And then we can assume that **in markets where we spend more on radio our sales will tend to be higher, and as our correlation matrix shows, we also tend to spend more on newspaper advertising in those same markets.** \n",
    "\n",
    "- Hence, in a simple linear regression which only examines sales versus newspaper, we'll observe that higher values of newspaper tend to be associated with higher values of sales, even though newspaper advertising does not actually affect sales. **Newspaper sales are a surrogate for radio advertising; newspaper gets \"credit\" for the effect of radio on sales.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some Important Questions to Asks when Perform Multiple Linear Regression\n",
    "\n",
    "## Is There a Relationship Between the Target and Predictors?\n",
    "\n",
    "### Step 1: hypothesis testing\n",
    "\n",
    "In simple linear regression, we simply check whether ${\\beta_1}$ = 0. In the multiple regression setting, we need to ask whether all of the regression coefficients are zero. As in the simple linear regression, we use a hypothesis test to answer this question. \n",
    "\n",
    "We test the **null hypothesis**\n",
    "\n",
    "```\n",
    "H_0 : β1 = β2 = · · · = βp = 0\n",
    "```\n",
    "versus the **alternative**\n",
    "```\n",
    "H_a : at least one βj is non-zero\n",
    "```\n",
    "\n",
    "This hypothesis test is performed by computing the **F-statistic**,\n",
    "\n",
    "\\begin{align}\n",
    "F=\\frac{(TSS-RSS)/p}{RSS/(n-p-1)}\n",
    "\\end{align}\n",
    "\n",
    "where $TSS =(y_i − \\bar{y})^2$ and $RSS =(y_i−\\hat{y}_i)^2$. \n",
    "\n",
    "> **TSS(total sum of squares)**: $\\sum_{i=1}^n(y_i-\\bar{y})^2$ - the total amount of variability inherently in the target variabe before the regression is performed\n",
    "\n",
    "> **RSS**: $e_1^2+e_2^2+e_3^2+...e_i^2$ = $\\sum_{i=1}^n(y_i-\\hat{y})^2$ - the amount of variability that is left unexplained after performing the regression\n",
    "\n",
    "> **TSS−RSS**: measures the amount of variability in the target variable that is explained (or removed) by performing the regression\n",
    "\n",
    "If the linear model assumptions are correct, one can show that\n",
    "\n",
    "\\begin{align}\n",
    "E[RSS/(n-p-1)]=\\sigma^2\n",
    "\\end{align}\n",
    "\n",
    "> The linear regression model general assumes the error ${e_i}$ is normally distributed, with zero mean and constant variance sigma^2. And if this assumption is true, \n",
    "$\\frac{\\sum_{i=1}^n(y_i-\\hat{y})^2}{n-p-1} = \\frac{RSS}{n-p-1}$ will the unbiased estimate of $\\sigma^2$\n",
    "\n",
    "and that, provided H0 is true,\n",
    "\n",
    "\\begin{align}\n",
    "E[(TSS-RSS)/p]=\\sigma^2\n",
    "\\end{align}\n",
    "\n",
    "> **TSS−RSS** is the amount of variablity in y that can be explained by ${\\beta_1}{x_1}$ to ${\\beta_p}{x_p}$, and therefore $(TSS-RSS)/p$ is the average amount of variablity in y that can be explained by each predictor. If this is equal to $\\sigma^2$, which is the inherent error in the target variable, it means that our dependent variables in the model don't have any perdicting power to the target variable.\n",
    "\n",
    "\n",
    "- When there is no relationship between the response and predictors,\n",
    "one would expect the F-statistic to take on a value close to 1. \n",
    "\n",
    "- On the other hand, if H_a is true, then $E[(TSS-RSS)/p]>\\sigma^2$, so we expect F to be\n",
    "greater than 1.\n",
    "\n",
    "- Then we can compute the p-value associated with the F-statistic and decide whether or not reject H_0. F-statistic follows an F-distribution. If the p-value is very close to 0, we'll have extremely strong evidence that at least one of the coefficient is associated with the target variable.\n",
    "\n",
    "<img src=\"images/6.png\" width=300>\n",
    "\n",
    "\n",
    "\n",
    "**If you want to test a particular subset of q of the coefficients are zero**\n",
    "\n",
    "This corresponds to a null hypothesis\n",
    "\n",
    "```\n",
    "H_0 : β(p-q+1) = β(p-q+2) = · · · = βp = 0\n",
    "```\n",
    "\n",
    "In this case we fit a second model that uses all the variables **except those last q**. Suppose that the residual sum of squares for that model is RSS_0. Then the appropriate F-statistic is\n",
    "\n",
    "\\begin{align}\n",
    "F=\\frac{(RSS_0-RSS)/q}{RSS/(n-p-1)}\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "**F-statistics v.s. t-statistics**\n",
    "\n",
    "- **Equivalency**: In Table 3.4, for each individual predictor a t-statistic and a p-value were reported. These provide information about whether each individual predictor is related to the response, after adjusting for the other predictors. It turns out that each of these are exactly equivalent  to the F-test that omits that single variable from the model, leaving all the others in—i.e. q=1 in the model. So it reports the **partial effect** of adding that variable to the model.\n",
    "\n",
    "> The square of each *t-statistic* is the corresponding *F-statistic*.\n",
    "\n",
    "- **p is large**: It seems likely that if any one of the p-values for the individual variables is very small, then at least one of the predictors is related to the response. However, this logic is flawed, especially when the number of predictors p is large.\n",
    " - For instance, consider an example in which p = 100 and H_0 : β1 = β2 = · · · = βp = 0 is true, so no variable is truly associated with the response. In this situation, about 5% of the p-values associated with each variable will be below 0.05 by chance. In other words, we expect to see approximately five small p-value even in the absence of any true association. Hence, if we use the individual t-statistics and associated p-values to decide whether there is any association between the variables and the response, high chance we will incorrectly conclude there is a relationship. \n",
    " - However, the F-statistic does not suffer from this problem because it adjusts for the number of predictors. If H_0 is true, there's only a 5% chance that the F-statistic will result in a p-value below 0.05, regardless of the number of predictors or the number of observations.\n",
    "\n",
    "- if p(number of predictors) > n (sample size): then there're more coefficients βj to estimate than observations from which to estimate them. In this case we cannot even fit the multiple linear regression model using least squares, so F-statistic cannot be used.\n",
    "\n",
    "### Step 2: decide on important variables\n",
    "If we conclude on the basis of that p-value that at least one of the predictors is related to the response, then it's natural to wonder which are the guiltones.\n",
    "We can perform variable selection by trying out a lot of different models, each containing a different subset of the predictors. There're 3 classical approaches for this:\n",
    "\n",
    "- **Forward selection:** we begin with the empty model (like for linear regression, begin with model contains an intercept but no predictors). We test the impact of adding variables one by one, using a chosen model fit criterion  for judgement(like adjusted R2 for regression model). We then add the variable whose inclusion gives the most significant improvement of the fit (like the variable which results in the lowest RSS), and repeating this process until some stopping rules is satisfied, like no variables can improve the model to a significant extent.\n",
    "\n",
    "- **Backward selection:** we start with all variables in the model, testing the deletion of each variable using a chosen model fit criterion. Delete the variable whose loss gives the most insignificant loss to the model fit. For linear regression, we can remove the variale with the largest p-value, re-fit the model and repeat the procedure until a stopping rule is reached, like all remaining varibales have a p-value below some threshold.\n",
    "\n",
    "- **Mixed selection:** this is a combination of forward and backward selection. We start with no variables in the moel, as with forward selection, we add the variable that provides the best fit and continue this procedure one-by-one. As we know that, like in linear regression, the p-value for some variables can become larger as new predictors are added. This is because forward selection is a **greedy approach**, and might include variables early that later bome redundant. We can remedy this by removing that variable from the model and continue to perform these forward and backward steps until all variables in the model have a sufficiently low p-value, and all variables outside the model would have a larg p-value if added to the model.\n",
    "\n",
    "\n",
    "\n",
    "- Various model fit criterion can be used to judge the quality of a regression model:\n",
    "\n",
    " - **Mallow’s Cp, Akaike information criterion (AIC)**\n",
    " - **Bayesian information criterion (BIC)**\n",
    " - **adjusted R2**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: decide on how well does the model fit the data?\n",
    "\n",
    "Two of the most common numerical measures of model fit are the **RSE** and **R2**\n",
    "\n",
    "#### R2 Statistics\n",
    "\n",
    "An $R^2$ value close to 1 indicates that the model explains a large portion of the variance in the response variable.\n",
    "\n",
    "\\begin{align}\n",
    "R^2 = (TSS − RSS)/TSS= 1− RSS/TSS\n",
    "\\end{align}\n",
    "\n",
    "> Recall that in simple regression, $R^2$ is the square of the correlation between the response and the variable. \n",
    "In multiple linear regression, it turns out that it equals $Cor(Y, \\hat{Y})^2$, the square of the correlation between the response and the fitted linear model; in fact one property of the fitted linear model is that it maximizes this correlation among all possible linear models.\n",
    "\n",
    "\n",
    "**$R^2$ will always increase when more variables are added to the model, even if those variables are only weakly associated with the response.**\n",
    "- This is due to the fact that adding another variable to the least squares equations must allow us to fit the training data (though not necessarily the testing data) more accurately. Thus, the $R^2$ statistic, which is computed on the training data, must increase.\n",
    "- Besides, if adding one predictor to the model leads to just a tiny increase in R2, this fact can provide some additional evidence that this varibale can be dropped from the model.\n",
    "\n",
    "\n",
    "#### RSE\n",
    "\n",
    "**RSE** is defined as\n",
    "\n",
    "\\begin{align}\n",
    "RSE=\\sqrt{\\frac{RSS}{n-p-1}}\n",
    "\\end{align}\n",
    "\n",
    "Models with more variables can have higher RSE if the decrease in RSS is small relative to the increase in p."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: how accurate is our prediction? \n",
    "\n",
    "To predict the response Y on the basis of a set of predictors, there're 3 sorts of **uncertainty associated with prediction**\n",
    "\n",
    "1. The coefficient estimates $\\hat{\\beta_0},\\hat{\\beta_1},...,\\hat{\\beta_p}$ are estimates for $β_0, β_1, . . . , β_p$. That is, the **least squares formula** is only an estimate for the **true population regression formula**. We can compute a **confidence interval** in order to determine how close $\\hat{Y}$ will be to f(X).\n",
    "\n",
    "> least squares formula\n",
    "\n",
    "\\begin{align}\n",
    "\\hat{Y}=\\hat{\\beta_0}+\\hat{\\beta_1}X_1+,...+\\hat{\\beta_p}X_p\n",
    "\\end{align}\n",
    "\n",
    "> true population regression formula\n",
    "\n",
    "\\begin{align}\n",
    "f(X)=\\beta_0+\\beta_1X_1+,...+\\beta_pX_p\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "2. In practice, assuming a linear model for f(X) is almost always an approximation of reality, so there is some potential **model bias**. When we use a linear model, we are in fact estimating the best linear approximation to the true surface.\n",
    "\n",
    "3. Even if we knew f(X)—true values for β0, β1, . . . , βp—the response value cannot be predicted perfectly because of the random error $\\epsilon$ --**irreducible error**. To answer the question of how much will Y vary from $\\hat{Y}$, we use **prediction intervals**.\n",
    "\n",
    "\n",
    "#### Prediction intervals\n",
    "\n",
    "**Prediction intervals** are always wider than **confidence intervals**\n",
    " - Because they incorporate both **the error in the estimate for f(X)** (the reducible error) and the **uncertainty as to how much an individual point will differ from the population regression plane** (the irreducible error).\n",
    "\n",
    "E.g. \n",
    "- **confidence interval** : quantify the uncertainty surrounding the average y over a large number of sample. For example, given that ${x_1}$ is 100,000, ${x_2}$ is 20,000, the 95% confidence interval for $y$ is [10,958, 11,528]. We interpret this to mean that there is a 95% probability that this interval will contain the true value of y.\n",
    "\n",
    "- **prediction interval** : quantify the uncertainty surrounding y for a sample. For example, given that ${x_1}$ is 100,000, ${x_2}$ is 20,000 in this sample, the 95% prediction interval for $y$ is [7,930, 14,580]. We interpret this to mean that there is a 95% probability that this interval will contain the true value of y for this specific sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A complete template for linear regression case\n",
    "\n",
    "## Is there a relationship between advertising sales and budget?\n",
    "\n",
    "- fitting a regression model\n",
    "- for simple linear regression, compute t-statistic and p-value; for multiple regression, compute F-statistic and p-value\n",
    "\n",
    "<img src=\"images/28.png\" width=600>\n",
    "\n",
    "## How strong is the relationship?\n",
    "- RSE\n",
    "- $\\sqrt{R}$\n",
    "\n",
    "<img src=\"images/29.png\" width=600>\n",
    "<img src=\"images/30.png\" width=600>\n",
    "\n",
    "## Which media contribute to sales?\n",
    "- examine p-values associated with each predictor's t-statistic\n",
    "\n",
    "<img src=\"images/31.png\" width=600>\n",
    "\n",
    "## How large is the effect of each medium on sale?\n",
    "- standard error of coefficients and confidence intervals for coefficients\n",
    "- width of confidence intervals. if CI is wide, considering collinearity.\n",
    "- confidence interval's distance from zero\n",
    "\n",
    "<img src=\"images/32.png\" width=600>\n",
    "\n",
    "## How accurately can we predict future sales?\n",
    "- prediction intervals\n",
    "\n",
    "<img src=\"images/33.png\" width=600>\n",
    "\n",
    "## Is the relationship linear?\n",
    "- residual plots should display no pattern for linear relation\n",
    "\n",
    "<img src=\"images/34.png\" width=600>\n",
    "\n",
    "## Is there synergy among the advertising media/\n",
    "- test p-value of interaction terms\n",
    "\n",
    "<img src=\"images/35.png\" width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Least-squares estimation\n",
    "\n",
    "Let's assume that the independent variable is $\\vec{x} = [x_1, x_2, \\dots, x_m]$ and the model's parameters are $\\vec{w} = [\\beta_0, \\beta_1, \\cdots, \\beta_m]$. \n",
    "\n",
    "Then the model's prediction would be $y \\approx \\beta_0 + \\sum_{i=1}^m \\beta_i\\times x_i$. If $\\vec{x}$ is extended to  $\\vec{x} = [1, x_1, x_2, \\dots, x_m]$ then $y$ would become a dot product of the parameter and the independent variable, i.e. $y \\approx \\sum_{i=0}^ m \\beta_i \\times x_i = \\vec{w} \\,\\,.\\, \\vec{x}$. \n",
    "\n",
    "Now putting the independent and dependent variables in matrices  <math>X</math> and <math>Y</math> respectively, the RSS function: $RSS =\\sum_{i=1}^n(y_n−\\vec{w}{x_n})^2$, which we need it to be minimized, can be rewritten in matrix notation as:\n",
    "\n",
    "<math>\n",
    "\\begin{align}\n",
    "L(D, \\vec{w})&= ||X\\vec{w} - Y||^2 \\\\&= (X\\vec{w} - Y)^T (X\\vec{w} - Y) \\\\ &= Y^{T}Y- Y^{T}X\\vec{w}- \\vec{w}^{T}X^TY+\\vec{w}^{T}X^TX\\vec{w}\n",
    "\\end{align}\n",
    "</math>\n",
    "\n",
    "Take the derivative of the above loss function with respect to $w$:\n",
    "\n",
    "<math>\n",
    "\\begin{align}\n",
    "\\frac{\\partial L(D, \\vec{w})}{\\partial \\vec{w}} &= \\frac{\\partial \\left(Y^{T}Y- Y^{T}X\\vec{w}- \\vec{w}^{T}X^TY+\\vec{w}^{T}X^TX\\vec{w}\\right)}{\\partial \\vec{w}} \\\\ &= -2Y^TX + 2\\vec{w}^TX^TX\n",
    "\\end{align}\n",
    "</math>\n",
    "\n",
    "Setting the derivative function to 0 and solve for $w$:\n",
    "\n",
    "<math>\n",
    "\\begin{align}\n",
    "-2Y^TX + 2\\vec{w}^TX^TX = 0 \\\\ &\\Rightarrow Y^TX  = \\vec{w}^TX^TX \\\\ &\\Rightarrow X^TY  = X^TX\\vec{w} \\\\ &\\Rightarrow \\vec{\\hat{w}} = (X^TX)^{-1}X^TY\n",
    "\\end{align}\n",
    "</math>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Linear Regression\n",
    "\n",
    "Input:      X_test: input array for prediction (1xN)\n",
    "            X_train: training data set of known vectors with size M (NxM)\n",
    "            y_train: target labels for training dataset (1xM vector)\n",
    "\n",
    "Output:     y_pred: predicted target value for X_test\n",
    "'''\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def regression(X_train, y_train, X_test):\n",
    "    # Load the x_train and X_test arrays and add the X0 value, which is 1 to every subarrays\n",
    "    X_train = np.insert(X_train, 0, 1, axis=1)\n",
    "    X_test = np.insert(X_test, 0, 1, axis=1)\n",
    "    \n",
    "    # Convert them into matrices\n",
    "    xMat = mat(X_train)\n",
    "    yMat = mat(y_train).T\n",
    "    xtestMat = mat(X_test)\n",
    "    \n",
    "    # Compute X multiplied by its transpose\n",
    "    xTx = np.dot(xMat.T, xMat)\n",
    "    \n",
    "    # If the determinant is 0, the matrix is invertible.\n",
    "    # The columns of X are linearly dependent, which means there're multi-collinearities\n",
    "    if linalg.det(xTx) == 0.0:\n",
    "        print (\"This is singular matrix\")\n",
    "        return\n",
    "    \n",
    "    # Inverse dot product of X and its transpose\n",
    "    temp1 = np.linalg.inv(xTx)\n",
    "    \n",
    "    # Multiplied by transpose X\n",
    "    temp2 = np.dot(temp1, xMat.T)\n",
    "    \n",
    "    # Multiplied by y\n",
    "    w = np.dot(temp2, yMat)\n",
    "    \n",
    "    # calculate prediction\n",
    "    y_pred = xtestMat * w\n",
    "    \n",
    "    print('The prediction for x_test is: ', y_pred)\n",
    "    return w, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The prediction for x_test is:  [[12.]]\n"
     ]
    }
   ],
   "source": [
    "# Simple linear regression test case\n",
    "X_train = np.array([[2],\n",
    "                    [4]])\n",
    "y_train = np.array([11, 12])\n",
    "X_test = np.array([[4]])\n",
    "\n",
    "_, y_pred = regression(X_train, y_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.plot(X_train, y_train)\n",
    "plt.plot(X_test, y_pred)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "301px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
