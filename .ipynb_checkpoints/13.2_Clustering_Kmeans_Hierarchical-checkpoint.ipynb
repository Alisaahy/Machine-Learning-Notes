{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering Methods\n",
    "\n",
    "Clustering refers to a very broad set of techniques for finding **subgroups**, or clusters, in a data set. When we cluster the observations of a data set, we seek to partition them into distinct groups so that \n",
    "- the observations within each group are quite similar to each other\n",
    "- while observations in different groups are quite different from each other.\n",
    "\n",
    "**Clustering vs PCA**\n",
    "\n",
    "Both clustering and PCA seek to simplify the data via a small number of summaries, but their mechanisms are different:\n",
    "• PCA looks to find a **low-dimensional representation of the observations** that explain a good fraction of the variance;\n",
    "• Clustering looks to find **homogeneous subgroups** among the observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Means Clustering\n",
    "\n",
    "## K-Means Clustering Algorithm\n",
    "\n",
    "The idea behind K-means clustering is that a good clustering is one for which the **within-cluster variation** is as small as possible.\n",
    "\n",
    "Suppse that we have a function $W(C_k)$ to measure within-cluster variation. Hence we want to solve the problem:\n",
    "\n",
    "\\begin{align}\n",
    "minimize_{C_1,...,C_k} {\\sum_{k=1}^KW(C_k)}\n",
    "\\end{align}\n",
    "\n",
    "which means, we want to partition the observations into K clusters such that the **total within-cluster variation, summed over all K clusters**, is as small as possible.\n",
    "\n",
    "\n",
    "Define the **within-cluster variation**, which is the sum of all of the pairwise squared Euclidean distances between the observations in the cluster, divided by the total number of observations in the cluster.\n",
    "\n",
    "\\begin{align}\n",
    "W(C_k) = \\frac{1}{|C_k|}\\sum_{i, i' \\in C_k} \\sum_{j=1}^p(x_{ij}-x_{i'j})^2 &&&& (10.11)\n",
    "\\end{align}\n",
    "\n",
    "where $|C_k|$ denotes the number of observations in the kth cluster.\n",
    "\n",
    "\n",
    "**Steps to solve the K-means problem**\n",
    "\n",
    "1. Randomly assign a number, from 1 to K, to each of the observations. These serve as **initial cluster assignments** for the observations.\n",
    "2. Iterate until the cluster assignments stop changing:\n",
    "    - For each of the K clusters, compute the **cluster centroid**. The cluster centroid is a vector that contains the mean of each features for all observations in that cluster.\n",
    "    - Assign each observation to the cluster whose centroid is closest (where closest is defined using Euclidean distance).\n",
    "    \n",
    "These steps are guaranteed to decrease the value of the objective (10.11) at each step. \n",
    "\n",
    "\n",
    "**Attention**\n",
    "\n",
    "Because the K-means algorithm finds a **local optimum** rather than a global optimum, the results obtained will depend on the initial (random) cluster assignment of each observation in Step 1. For this reason, it is important to run the algorithm multiple times from **different random initial configurations**. Then one selects the best solution, i.e. that for which the **within-cluster variation** is smallest.\n",
    "\n",
    "\n",
    "<img src=\"./images/108.png\" width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elbow method\n",
    "\n",
    "The Elbow Method is one of the most popular methods to determine this optimal value of k in K-means clustering. \n",
    "\n",
    "An elbow plot shows at each value of k, what is the sum of squared distance of samples to their assigned cluster center. \n",
    "\n",
    "\n",
    "**Elbow point**\n",
    "\n",
    "It's true that increasing the number of clusters will naturally decrease the sum of squared distance, but more clusters lead to higher chances of subdividing the labeled groups into multiple clusters. Therefore, we want to choose a number of clusters so that adding another cluster doesn't give much information of the data, which is the point the drop in the sum of squared distance starts to slow down.\n",
    "\n",
    "\n",
    "<img src=\"./images/109.png\" width=500>\n",
    "\n",
    "\n",
    "\n",
    "## Silhouette coefficient\n",
    "\n",
    "The Silhouette Coefficient is calculated using the mean within-cluster distance (a) and the mean nearest-cluster distance (b) for each sample. The Silhouette Coefficient for a sample is (b - a) / max(a, b). We can compute the mean Silhouette Coefficient over all samples and use this as a metric to judge the number of clusters. We want this coefficient to be larger.\n",
    "\n",
    "\n",
    "**Elbow vs Silhouette**\n",
    "\n",
    "Silhouette coefficient exhibits a peak characteristic as compared to the gentle bend in the elbow method. This is easier to visualize and reason with.\n",
    "\n",
    "\n",
    "<img src=\"./images/110.png\" width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hierarchical Clustering\n",
    "\n",
    "Hierarchical clustering has an added advantage over K-means clustering in that it results in an attractive **tree-based representation** of the observations, called a **dendrogram**.\n",
    "\n",
    "## Interpreting a Dendrogram\n",
    "\n",
    "- Each leaf of the dendrogram represents one of the observations\n",
    "- As we move up the tree, some leaves or branches begin to fuse into branches. These correspond to observations or groups of observations that are similar to each other\n",
    "    - The earlier (lower in the tree) fusions occur, the more similar the groups of observations are to each other.\n",
    "    - Observations that fuse later (near the top of the tree) can be quite different.\n",
    "    \n",
    "- The height of this fusion, as measured on the vertical axis, indicates how **different** the two observations are.\n",
    "\n",
    "\n",
    "**Attention**\n",
    "\n",
    "<img src=\"./images/111.png\" width=650>\n",
    "\n",
    "\n",
    "We cannot draw conclusions about the similarity of two observations based on their proximity along the horizontal axis. Rather, we draw conclusions about the similarity of two observations based on the location on the vertical axis where branches containing those two observations **first are fused**.\n",
    "\n",
    "**Decide where to cut**\n",
    "\n",
    "<img src=\"./images/112.png\" width=650>\n",
    "\n",
    "\n",
    "In order to identify clusters, we make a horizontal cut across the dendrogram. The distinct sets of observations beneath the cut can be interpreted as clusters. The height of the cut to the dendrogram controls the number of clusters obtained.\n",
    "\n",
    "In practice, people often look at the dendrogram and select by eye a sensible number of clusters, based on the heights of the fusion and the number of clusters desired\n",
    "\n",
    "\n",
    "## Hierarchical vs Kmeans\n",
    "\n",
    "Hierarchical clustering create a complete range of n**ested cluster solutions**. Clusters obtained by cutting the dendrogram at a given height are necessarily nested within the clusters obtained by cutting the dendrogram at any greater height. However, on an arbitrary data set, true clusters are **not nested**. Consequently, this situation could not be well-represented by hierarchical clustering.\n",
    "\n",
    "\n",
    "## The Hierarchical Clustering Algorithm\n",
    "1. Begin with n observations and a measure (such as Euclidean distance) of all the 􏰁$\\begin{pmatrix}n \\\\ 2\\end{pmatrix}$ = n(n − 1)/2 pairwise dissimilarities. Treat each 2 observation as its own cluster.\n",
    "\n",
    "2. For i = n, n−1, ...,2:\n",
    "    - Examine all pairwise **inter-cluster dissimilarities** among the i clusters and identify the pair of clusters that are **least dissimilar** (**most similar**). Fuse these two clusters. The dissimilarity between these two clusters indicates the height in the dendrogram at which the fusion should be placed.\n",
    "    - Compute the new **pairwise inter-cluster dissimilarities** among the i − 1 remaining clusters.\n",
    "\n",
    "<img src=\"./images/114.png\" width=650>\n",
    "\n",
    "\n",
    "    \n",
    "## Linkage   \n",
    "**How do we define the pairwise dissimilarity between groups of observations?**\n",
    "\n",
    "There're four most common types of linkage—complete, average, single, and centroid. \n",
    "\n",
    "<img src=\"./images/113.png\" width=650>\n",
    "\n",
    "\n",
    "- **Average** and **complete** linkage are generally preferred over single linkage, as they tend to yield more balanced dendrograms.\n",
    "\n",
    "- Centroid linkage is often used in genomics, but suffers from a major drawback in that an **inversion** can occur, whereby two clusters are fused at a height below either of the individual clusters in the dendrogram. This can lead to difficulties in visualization as well as in interpretation of the dendrogram.\n",
    "\n",
    "- The resulting dendrogram typically depends quite strongly on the type of linkage used.\n",
    "\n",
    "\n",
    "## Choice of Dissimilarity Measure\n",
    "\n",
    "- **Euclidean distance**\n",
    "\n",
    "- **Correlation-based distance**: considers two **observations** to be similar if their features are **highly correlated**, even though the observed values may be far apart in terms of Euclidean distance. It focuses on the **shapes of observation profiles** rather than their magnitudes. (This is an unusual use of correlation, which is normally computed between variables)\n",
    "\n",
    "- The choice of dissimilarity measure is very important, as it has a strong effect on the resulting dendrogram.\n",
    "\n",
    "> For instance, consider an online retailer interested in clustering shoppers based on their past shopping histories. The goal is to identify subgroups of similar shoppers.  \n",
    "\n",
    "> Suppose the data takes the form of a matrix where the rows are the shoppers and the columns are the items available for purchase; the elements of the data matrix indicate the number of times a given shopper has purchased a given item.\n",
    "\n",
    "> - If Euclidean distance is used, then shoppers who have bought very few items overall (i.e. infrequent users of the online shopping site) will be clustered together. This may not be desirable. \n",
    "> -  If correlation-based distance is used, then shoppers with similar preferences (e.g. shoppers who have bought items A and B but never items C or D) will be clustered together, even if some shoppers with these preferences are higher-volume shoppers than others. Therefore, for this application, correlation-based distance may be a better choice.\n",
    "\n",
    "- Also consider whether or not the variables should be normalized before the dissimilarity between the observations is computed.\n",
    "\n",
    "> High-frequency purchases like socks therefore tend to have a much larger effect on the inter-shopper dissimilarities, and hence on the clustering ultimately obtained, than rare purchases like computers. This may not be desirable.\n",
    "\n",
    "> - If the variables are scaled before the inter-observation dissimilarities are computed, then each variable will in effect be given equal importance in the hierarchical clustering performed.\n",
    "\n",
    "> - We might also want to scale the variables if they are measured on different scales; otherwise, the choice of units (e.g. centimeters versus kilometers) for a particular variable will greatly affect the dissimilarity measure obtained. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical Issues in Clustering\n",
    "\n",
    "- Should the observations or features first be standardized in some way?\n",
    "\n",
    "- In the case of hierarchical clustering,\n",
    "    - What dissimilarity measure should be used?\n",
    "    - What type of linkage should be used?\n",
    "    - Where should we cut the dendrogram in order to obtain clusters?\n",
    "\n",
    "- In the case of K-means clustering, how many clusters should we look for in the data?\n",
    "\n",
    "\n",
    "Each of these decisions can have a strong impact on the results obtained. In practice, there is no single right answer. We **try several different choices**, and look for the one with the most useful or interpretable solution.\n",
    "\n",
    "\n",
    "# Other Considerations: Outliers / perturbations\n",
    "\n",
    "Both K-means and hierarchical clustering will assign each observation to a cluster. However, sometimes this might not be appropriate. For instance, suppose that most of the observations truly belong to a small number of (unknown) subgroups, and a **small subset** of the observations are **quite different** from each other and from all other observations. Then since K- means and hierarchical clustering force every observation into a cluster, the clusters found may be heavily distorted due to the presence of outliers that do not belong to any cluster.\n",
    "\n",
    "\n",
    "We can try **a soft version of K-means clustering**, and are described in Hastie et al. (2009).\n",
    "\n",
    "\n",
    "In addition, clustering methods generally are not very robust to perturbations to the data. For instance, suppose that we cluster n observations, and then cluster the observations again after removing a subset of the n observations at random. One would hope that the two sets of clusters ob- tained would be quite similar, but often this is not the case! Since clustering can be non-robust, we recommend clustering subsets of the data in order to get a sense of the robustness of the clusters obtained. \n",
    "\n",
    "\n",
    "**Clustering results should not be taken as the absolute truth about a data set. They should constitute a starting point for the development of a scientific hypothesis and further study, preferably on an independent data set.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Supervised Learning to Generate Cluster Descriptions\n",
    "\n",
    "However clustering was done, it provides us with a list of assignments indicating which examples belong to which cluster. **A cluster centroid**, in effect, describes the average cluster member. \n",
    "\n",
    "***The problem is that these descriptions may be very detailed and they don’t tell us how the clusters differ.***\n",
    "\n",
    "\n",
    "The general strategy is this: we use the cluster assignments to label examples. Once we have a labeled set of examples, we run a supervised learning algorithm on the example set to generate a classifier for each class/cluster. We can then inspect the classifier descriptions to get a (hopefully) intelligible and concise description of the corresponding cluster. \n",
    "\n",
    "\n",
    "\n",
    "**Example**: \n",
    "\n",
    "1. We want to know what makes the cluster J differentiate with other clusters. So we go back to our raw data and label the datset as **J** and **Not_J** for each observation, indicating whether it was assigned to the J cluster.\n",
    "\n",
    "2. We pass the dataset to the decision tree model, and see how this tree builds rules to differentiate J and not J.\n",
    "\n",
    "<img src=\"./images/119.png\" width=650>\n",
    "\n",
    "Tracing paths from the root to these leaves, we can extract the two rules:\n",
    "1. (ROUND_BODY = 1) AND (NOSE_SHERRY = 1) ⇒ J\n",
    "2. (ROUND_BODY = 0) AND (color_red = 0) AND (color_f.gold = 1) AND\n",
    "(BODY_light = 1) AND (FIN_dry = 1) ⇒ J"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example\n",
    "\n",
    "\n",
    "**Unsupervised problems often are much more exploratory. We could understand our business better, and therefore be able to improve something.**\n",
    "\n",
    "For clustering, specifically, it often is difficult even to understand what (if anything) the clustering reveals. Even when the clustering does seem to reveal interesting information, it often is not clear how to use that to make better decisions. Therefore, for clustering, additional creativity and business knowledge must be applied in the Evaluation stage of the data mining process\n",
    "\n",
    "- Ira Haimowitz and Henry Schwartz (1997) show a concrete example of **how clustering was used to improve decisions about how to set credit lines for new credit customers**.\n",
    "\n",
    "- They clustered existing GE Capital customers based on **similarity in their use of their cards, payment of their bills, and profitability to the company**. After some work, they settled on five clusters that represented very different consumer credit behavior (e.g., those who spend a lot but **pay off their cards in full each month** versus those who spend a lot and **keep their balance near their credit limit**). These different sorts of customers can tolerate very different credit lines (in the two examples, extra care must be taken with **the latter to avoid default**). \n",
    "\n",
    "- The problem with using this clustering immediately for decision making is that **the data are not available when the initial credit line is set**. Briefly, Haimowitz and Schwarz took this new knowledge and cycled back to the beginning of the data mining process. They used the knowledge to define a precise predictive mod‐ eling problem: **using data that are available at the time of credit approval, predict the probability that a customer will fall into each of these clusters**. This predictive model then can be used to improve initial credit line decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Means Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here’s how the pseudo-code would look:\n",
    "\n",
    "Create k points for starting centroids (often randomly) \n",
    "\n",
    "While any point has changed cluster assignment:\n",
    "> for every point in our dataset: \n",
    ">> for every centroid:\n",
    ">>> calculate the distance between the centroid and point \n",
    "\n",
    ">> assign the point to the cluster with the lowest distance\n",
    "\n",
    "> for every cluster:\n",
    ">> calculate the mean of the points in that cluster and assign the centroid to the mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "\n",
    "data = datasets.load_iris().data\n",
    "\n",
    "def euclDistance(vector1, vector2):\n",
    "    return np.sqrt(np.sum((vector1-vector2)**2))\n",
    "\n",
    "def initCentroids(data, k):\n",
    "    numSamples, dim = data.shape\n",
    "    centroids = zeros((k, dim))\n",
    "    for i in range(k):\n",
    "        # Randomly select data index in the dataset\n",
    "        index = int(random.uniform(0, numSamples))\n",
    "        \n",
    "        # Assign the random data point as centroids\n",
    "        centroids[i,:] = data[index,:]\n",
    "    return centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans(data, k):\n",
    "    numSamples = data.shape[0]\n",
    "    # Create cluster matrix for each sample\n",
    "    # first column stores which cluster this sample belongs to,\n",
    "    # second column stores the error, which is the distance from the cluster centroid to the current point.\n",
    "    clusterAssment = mat(zeros((numSamples, 2)))\n",
    "    clusterChanged = True\n",
    "\n",
    "    ## step 1: initiate centroids\n",
    "    centroids = initCentroids(data, k)\n",
    "\n",
    "    ## keep updating until the cluster assignments stop changing. \n",
    "    ## create a flag called clusterChanged, and if this is True you continue iterating\n",
    "    while clusterChanged:\n",
    "        clusterChanged = False\n",
    "    \n",
    "        ## for each sample\n",
    "        for i in range(numSamples):\n",
    "            minDist  = float('inf')\n",
    "            minIndex = 0\n",
    "        \n",
    "            ## for each centroid\n",
    "            ## step 2: find the centroid who is closest\n",
    "            for j in range(k):\n",
    "                distance = euclDistance(centroids[j], data[i])\n",
    "                if distance < minDist:\n",
    "                    minDist  = distance\n",
    "                    minIndex = j\n",
    "\n",
    "            ## step 3: update its cluster\n",
    "            if clusterAssment[i, 0] != minIndex:\n",
    "                clusterChanged = True\n",
    "                clusterAssment[i] = minIndex, minDist\n",
    " \n",
    "        ## step 4: update centroids\n",
    "        for j in range(k):\n",
    "            # get all the point in this cluster\n",
    "            # nonzero: return the indices of the elements that are true/non-zero.\n",
    "            # nonzero(clusterAssment[:, 0] == j): return the indices of the sample which is assigned to cluster j\n",
    "            # nonzero(clusterAssment[:, 0] == j)[0]: return only the row indices\n",
    "            pointsInCluster = data[nonzero(clusterAssment[:, 0] == j)[0]]\n",
    "            \n",
    "            #assign centroid to mean point in the custer\n",
    "            centroids[j] = mean(pointsInCluster, axis = 0)\n",
    "            \n",
    "    return centroids, clusterAssment"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "288px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
