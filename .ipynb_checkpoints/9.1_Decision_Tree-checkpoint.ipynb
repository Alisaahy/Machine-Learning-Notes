{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression Trees\n",
    "\n",
    "<img src=\"./images/87.png\" width=\"600\">\n",
    "<img src=\"./images/88.png\" width=\"600\">\n",
    "\n",
    "\n",
    "**Terminal nodes**: The leaves are at the bottom of the tree (the regions R1, R2, and R3) are known as terminal nodes or leaves of the tree. \n",
    "\n",
    "**Internal nodes**: The points along the tree where the predictor space is split (Years<4.5 and Hits<117.5) are referred to as internal nodes.\n",
    "\n",
    "**Branches**: The segments of the trees that connect the nodes as branches.\n",
    "\n",
    "\n",
    "**Advantages**\n",
    "- Easier to interpret, and has a nice graphical representation.\n",
    "\n",
    "\n",
    "**Interpretation**\n",
    "- Years is the most important factor in determining Salary, and players with less experience earn lower salaries than more experienced players. Given that a player is less experienced, the number of hits that he made in the previous year seems to play little role in his salary.\n",
    "- But among players who have been in the major leagues for five or more years, the number of hits made in the previous year does affect salary, and players who made more hits last year tend to have higher salaries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process of building a regression tree\n",
    "\n",
    "**Step 1**: We divide the predictor space—that is, the set of possible values for X1, X2,..., Xp into J distinct and non-overlapping regions, R1, R2,..., RJ. For every observation that falls into the region Rj, we make the same prediction, which is simply the **mean of the response values** for the training observations in Rj.\n",
    "\n",
    "**Step 2**: Apply cost complexity pruning to the large tree in order to obtain a sequence of best subtrees, as a function of $α$.\n",
    "\n",
    "**Step 3**: Use K-fold cross-validation to choose $α$. That is, divide the training observations into K folds. For each k = 1, . . . , K:\n",
    " - Repeat Step 1 and 2 on all but the kth fold of the training data.\n",
    " - Evaluate the mean squared prediction error on the data in the left-out kth fold, as a function of $α$.\n",
    " - Average the results for each value of $α$, and pick α to minimize the average error.\n",
    "\n",
    "**Step 4**: Return the subtree from Step 2 that corresponds to the chosen value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Recursive Binary Splitting\n",
    "\n",
    "**How do we construct the regions R1, R2,..., RJ?** \n",
    "\n",
    "**Recursive Binary Splitting**: a *top-down, greedy* approach to divide the predictor space\n",
    "- **Top-down**: begins at the top of the tree (at which point all observations belong to a single region) and then successively splits the predictor space; each split is indicated via two new branches further down on the tree. \n",
    "\n",
    "- **Greedy**: at each step of the tree-building process, the best split is made at that particular step, rather than looking ahead and picking a split that will lead to a better tree in some future step.\n",
    "\n",
    "\n",
    "**How to perform**:\n",
    "1. Select the predictor $X_j$ and the cutpoint $s$ such that splitting the predictor space into the regions ${X|X_j < s}$ and ${X|X_j ≥ s}$ leads to the greatest possible reduction in RSS\n",
    " - In greater detail, for any $j$ and $s$, we define the pair of half-planes\n",
    " \n",
    "\\begin{align}\n",
    "R_1(j, s) = {X|X_j < s} ,\\quad R_2(j, s) = {X|X_j ≥ s}\n",
    "\\end{align}\n",
    "\n",
    "and we seek the value of $j$ and $s$ that **minimize** the equation\n",
    "\n",
    "\\begin{align}\n",
    "\\sum_{:x_i \\in R_1(j,s)}(y_i-\\hat{y}_{R_1})^2+\\sum_{:x_i \\in R_2(j,s)}(y_i-\\hat{y}_{R_2})^2\n",
    "\\end{align}\n",
    "\n",
    "where $\\hat{y}_{R_1}$is the mean response for the training observations in $R_1(j, s)$,\n",
    "\n",
    "\n",
    "2. Repeat the process, looking for the best predictor and best cutpoint in order to split the data further so as to minimize the RSS within each of the resulting regions. \n",
    " \n",
    "\n",
    "3. The process continues until a stopping criterion is reached; for instance, we may continue until no region contains more than five observations.\n",
    "\n",
    "\n",
    "4. Once the regions R1,..., RJ have been created,we predict the response for a given test observation using the mean of the training observations in the region to which that test observation belongs."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Tree Pruning\n",
    "\n",
    "**Why pruning**\n",
    "\n",
    "The resulting tree might be so complex that can overfit. A smaller tree with fewer splits might lead to **lower variance** and **better interpretation** at the cost of a little bias.\n",
    "\n",
    "\n",
    "**Why not split the tree only if the the decrease in the RSS exceeds some threshold?**\n",
    "\n",
    "This strategy is too **short-sighted** since a seemingly worthless split early on in the tree might be followed by a very god split—that is, a split that leads to a large reduction in RSS later on.\n",
    "\n",
    "\n",
    "**Better strategy**\n",
    "A better strategy is to grow a very large tree $T_0$, and then **prune** it back in order to obtain a **subtree**\n",
    "\n",
    "\n",
    "### Cost complexity pruning / Weakest link pruning\n",
    "\n",
    "For each value of α there corresponds a subtree $T ⊂ T_0$ such that\n",
    "\n",
    "\\begin{align}\n",
    "\\sum_{m=1}^T\\sum_{i:x_i \\in R_m}(y_i − \\hat{y}_{R_m})^2 + \\alpha|T|  \\quad \\quad (8.4)\n",
    "\\end{align}\n",
    "\n",
    "is as small as possible. \n",
    "\n",
    "- $|T|$: the number of leaves of the tree T, \n",
    "- $R_m$: the subset of predictor space corresponding to the m_th **terminal node**, \n",
    "- $\\hat{y}_{R_m}$: the predicted response associated with $R_m$—that is, the mean of the training observations in $R_m$.\n",
    "\n",
    "\n",
    "The tuning parameter $α$ controls a *trade-off* between the subtree’s **complexity** and its **fit to the training data**. When α = 0, then the subtree T will simply equal T0, because then (8.4) just measures the training error.\n",
    "\n",
    "However, as $α$ increases, there is a price to pay for having a tree with many terminal nodes, and so the quantity (8.4) will tend to be minimized for a smaller subtree. \n",
    "\n",
    "We can select a value of α using a validation set or using cross-validation. \n",
    "\n",
    "<img src=\"./images/89.png\" width=600>\n",
    "<img src=\"./images/90.png\" width=600>\n",
    "\n",
    "\n",
    "### Reduced error pruning\n",
    "\n",
    "One of the simplest forms of pruning is reduced error pruning. Starting at the leaves, each node is replaced with **its most popular class**. **If the prediction accuracy is not affected** then the change is kept. While somewhat naive, reduced error pruning has the advantage of simplicity and speed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Trees\n",
    "\n",
    "For a classification tree, we predict that each observation belongs to the **most commonly occurring class** of training observations in the region to which it belongs.\n",
    "\n",
    "In the classification setting, RSS cannot be used as a criterion for making the binary splits. We can use classification error rate, gini index/cross-entropy.\n",
    "\n",
    "\n",
    "\n",
    "## Classification Error Rate\n",
    "\n",
    "Since we plan to assign an observation in a given region to the most commonly occurring class of training observations in that region, the classification error rate is simply the **fraction of the training observations in that region that do not belong to the most common class**:\n",
    "\n",
    "\\begin{align}\n",
    "E=1-\\max_k(\\hat{p}_{mk})\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "- $\\hat{p}_{mk}$ : the proportion of training observations in the mth region that are from the kth class.\n",
    "\n",
    "- classification error is not sufficiently sensitive for tree-growing, and in practice two other measures are preferable: **Gini index, cross-entropy.**\n",
    "\n",
    "\n",
    "\n",
    "## Gini index\n",
    "\n",
    "A measure of **total variance across the K classes**. It is not hard to see that the Gini index takes on a small value if the proportion of training observations $\\hat{p}_{mk}$ among all the classes are close to zero or one.\n",
    "\n",
    "\\begin{align}\n",
    "G=\\sum_{k=1}^K\\hat{p}_{mk}(1-\\hat{p}_{mk})\n",
    "\\end{align}\n",
    "\n",
    "The Gini index is referred to as a measure of node **purity**—a small value indicates that a node contains predominantly observations from a single class.\n",
    "\n",
    "\n",
    "\n",
    "## Cross-Entropy\n",
    "\n",
    "\\begin{align}\n",
    "D=-\\sum_{k=1}^K\\hat{p}_{mk}\\log{\\hat{p}_{mk}}\n",
    "\\end{align}\n",
    "\n",
    "- Since 0 ≤ $\\hat{p}_{mk}$ ≤ 1, it follows that $0 ≤ −\\hat{p}_{mk}\\log{\\hat{p}_{mk}}$. \n",
    "- Cross-entropy will take on a value near zero if the $\\hat{p}_{mk}$’s are all near zero or near one. Therefore, like the Gini index, the cross-entropy will take on a small value if the mth node is **pure**.\n",
    "\n",
    "\n",
    "------\n",
    "\n",
    "**Cross-Entropy v.s. Gini index v.s. Classification Error Rate**\n",
    "- When growing a classification tree (recursive binary splitting), either the **Gini index** or the **cross entropy** are typically used to evaluate the quality of a particular split, since these two approaches are more **sensitive to node purity** than is the classification error rate. \n",
    "- Any of these three approaches can be used when pruning the tree, but the classification error rate is preferable if prediction accuracy of the final pruned tree is the goal.\n",
    "\n",
    "<img src=\"./images/91.png\" width=800>\n",
    "\n",
    "- **A surprising characteristic**: some of the splits yield two terminal nodes that have the same predicted value. \n",
    " - **Why is the split performed at all?** The split is performed because it leads to **increased node purity.**\n",
    " - **Why is node purity important?** Suppose that we have a test observation that belongs to the region given by that right-hand leaf. Then we can be pretty certain that its response value is Yes. In contrast, if a test observation belongs to the region given by the left-hand leaf, then its response value is probably Yes, but we are much less certain. Even though the split RestECG<1 does not reduce the classification error, it improves the **Gini index and the cross-entropy**, which are more sensitive to node purity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trees Versus Linear Models \n",
    "\n",
    "Linear regression assumes a model of the form\n",
    "\\begin{align}\n",
    "f(X)=\\beta_0+\\sum_{i=1}^p\\beta_iX_i\n",
    "\\end{align}\n",
    "\n",
    "Regression trees assume a model of the form\n",
    "\\begin{align}\n",
    "f(X)=\\sum_{m=1}^Mc_m \\cdot I_{X \\in R_m}\n",
    "\\end{align}\n",
    "where R1, . . .,RM represent a partition of feature space\n",
    "\n",
    "**Linear regression works better**: If the relationship between the features and the response is well approximated\n",
    "by a linear model; regression tree does not exploit this linear structure. \n",
    "\n",
    "**Regression tree works better**: \n",
    "If instead there is a highly non-linear and complex relationship between the features and the response. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advantages and Disadvantages of Trees \n",
    "\n",
    "**Advantages of decision trees for regression and classification:**\n",
    "\n",
    "▲ **Interpretation and visualization**: Trees are very **easy to explain** to people. In fact, they are even easier to explain than linear regression! Trees can also be **displayed graphically**, and are easily interpreted even by\n",
    "a non-expert. Therefore, in certain settings, prediction using a tree may be preferred for the sake of **interpretability and visualization**.\n",
    "\n",
    "\n",
    "▲ Some people believe that decision trees more closely **mirror human decision-making** than do the regression and classification approaches.\n",
    "\n",
    "\n",
    "▲ Trees can easily handle qualitative predictors without the need to create dummy variables. (It really depends on algorithm. Dcision trees used in popular Python packages (scikit-learn and XGBoost) can't handle categorical data out of the box (scikit-learn for example uses CART algorithm). In H2O, we don't need to create dummy variabels)\n",
    "\n",
    "- Why not dummy?\n",
    "\n",
    "Imagine our categorical variable has 100 levels, each appearing about as often as the others. If we one-hot encoding this variable, all these variables will be transformed to sparse binary variables. And then, even the best algorithm can only reduce impurity by ≈ 1% by splitting on one of its dummies. Therefore, the algorithm may instead choose continuous variables.\n",
    " - The resulting sparsity virtually ensures that continuous variables are assigned higher feature importance.\n",
    " - A single level of a categorical variable must meet a very high bar in order to be selected for splitting early in the tree building. This can degrade predictive performance.\n",
    "\n",
    "\n",
    "\n",
    "**Disadvantages of decision trees for regression and classification:**\n",
    "\n",
    "▼ Unfortunately, trees generally do not have the same level of predictive accuracy as some of the other regression and classification approaches.\n",
    "\n",
    "▼ Additionally, trees can be very **non-robust**. In other words, a small change in the data can cause a large change in the final estimated tree.\n",
    "\n",
    "- However, by **aggregating many decision trees**, using methods like bagging, random forests, and boosting, the predictive performance of trees can be substantially improved. We introduce these concepts in the next section."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
