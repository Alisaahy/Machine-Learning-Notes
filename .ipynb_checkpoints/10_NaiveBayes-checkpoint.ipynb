{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes\n",
    "\n",
    "\n",
    "**Naïve Bayes Classifier** is a conditional probability model with the **maximum a posteriori or MAP decision rule**, which assigns a class label $\\hat{y} = C_i$ for an observation X, if its $p(C_i|X)$ is the highest among all k classes.\n",
    "\n",
    "**Assumption**: \n",
    "\n",
    "- independence among predictors\n",
    "- the presence of a particular feature in a class is unrelated to the presence of any other feature.\n",
    "\n",
    "\n",
    "1. The instance probability we want to derive is $p(C_k|X) = p(C_k|x_1, x_2, ..., x_n)$, which is the posterior probability of class ($C_k$, target) given predictor vector $X = (x_1, x_2, ..., x_n)$. According to the Bayes Theorem, the posterior probability can be represented as:\n",
    "\n",
    "\\begin{align}\n",
    "p(C_k|X) = \\frac{p(X|C_k)p(C_k)}{p(X)} \n",
    "\\end{align}\n",
    "\n",
    "\n",
    "2. Because the denominator does not depend on C and the values of the features X are given, so that the denominator is effectively constant. Therefore,\n",
    "\n",
    "\\begin{align}\n",
    "p(C_k|X) \\propto p(X|C_k)p(C_k)\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "3. If we insert $X = (x_1, x_2, ..., x_n)$ to the formular and apply the **chain rule**\n",
    "\n",
    "\\begin{align}\n",
    "p(C_k|x_1, x_2, ..., x_n) &\\propto p(x_1, x_2, ..., x_n|C_k)p(C_k) \\\\\n",
    "&= p(x_1, x_2, ..., x_n,C_k)\\\\\n",
    "&= p(x_1| x_2, ..., x_n,C_k)p(x_2, x_3, ..., C_k) \\\\\n",
    "&= p(x_1| x_2, ..., x_n,C_k)p(x_2| x_3, ..., C_k)p(x_3, ..., C_k) \\\\\n",
    "&= ... \\\\\n",
    "&= p(x_1| x_2, ..., x_n,C_k)p(x_2| x_3, ..., C_k)p(x_3|x_4, ..., C_k) ... p(x_{n-1}|x_n, C_k)p(x_n|C_k)p(C_k)\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "4. Now the \"naïve\" conditional independence assumptions come into play: because that all features in X are mutually independent,\n",
    "\n",
    "\\begin{align}\n",
    "p(x_i|x_{i+1}, ..., x_n,C_k) = p(x_i|C_K)\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "5. Thus, the posterior probability can be expressed as\n",
    "\n",
    "\\begin{align}\n",
    "p(C_k|x_1, x_2, ..., x_n) &\\propto p(x_1, x_2, ..., x_n,C_k) \\\\\n",
    "&=p(x_1| x_2, ..., x_n,C_k)p(x_2| x_3, ..., C_k)p(x_3|x_4, ..., C_k) ... p(x_{n-1}|x_n, C_k)p(x_n|C_k)p(C_k) \\\\\n",
    "&=p(x_1|C_k)p(x_2|C_k)p(x_3|C_k) ... p(x_n|C_k)p(C_k) \\\\\n",
    "&=p(C_k)\\prod_{i=1}^np(x_i|C_k)\n",
    "\\end{align}\n",
    "\n",
    "6. Construct a classifier\n",
    "\n",
    "\\begin{align}\n",
    "\\hat{y} = \\underset{k \\in \\{1, \\ldots, K\\}}{\\operatorname{argmax}} \\ p(C_k) \\displaystyle\\prod_{i=1}^n p(x_i \\mid C_k) &&&& \\text{where $p(C_k)$ is the probability of each class in the dataset}\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "7. In the final step, we are good to go: simply calculate the posterior probability for every class of a specific observation and compare which class has a higher probability. But to calculate each $p(x_i|C_k)$, we can't simply use a frequency-based probability. When a frequency-based probability is zero, it will wipe out all the information in the other probabilities. A solution would be **Laplace smoothing**, which is a technique for smoothing categorical data. With the lapalce smoothing. The resulting estimate will be between the empirical probability (relative frequency) $\\frac{x_i}{N}$, and the uniform probability $\\frac{1}{d}$. Usually, we use $\\alpha = 1$\n",
    "\n",
    "\\begin{align}\n",
    "p_{x_i}=\\frac{x_i+\\alpha}{N+\\alpha d} &&&& (i = 1,..., d)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Types of Naive Bayes\n",
    "\n",
    "## Multinomial Naive Bayes\n",
    "\n",
    "This is mostly used for **document classification** problem, i.e whether a document belongs to the category of sports, politics, technology etc. We can use the frequency of the words present in the document as predictors. To make your classifier more advanced, tf-idf is also an ideal option . Tf-idf not only counts the occurrence of a word in the given text(or document), but also reflect **inverse document frequency**, which is a measure of how much information the word provides. \n",
    "\n",
    "\n",
    "## Bernoulli Naive Bayes\n",
    "\n",
    "This is similar to the multinomial naive bayes but the predictors are **boolean variables**. The parameters that we use to predict the class variable take up only values yes or no, for example if a word occurs in the text or not.\n",
    "\n",
    "\n",
    "## Gaussian Naive Bayes\n",
    "\n",
    "When the predictors take up a **continuous** value and are not discrete, we assume that these values are sampled from a gaussian distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advantages and Disadvantages of Naive Bayes\n",
    "\n",
    "## Advantages\n",
    "\n",
    "1. In spite of its simplicity and the strict independence assumptions, the **Naive Bayes classifier performs surprisingly well** for classification on many real-world tasks and you **need less training data**. This is because the violation of the independence assumption tends not to hurt classification performance.\n",
    "\n",
    "\n",
    "2. Naive Bayes is that it is naturally an **“incremental learner”.** An incremental learner is an induction technique that can update its model one training example at a time. It does not need to reprocess all past training examples when new training data become available.\n",
    "\n",
    "\n",
    "3. It is easy and fast to predict class of test data set.\n",
    "\n",
    "## Disadvantages\n",
    "\n",
    "1. The probability estimation from Naive Bayes will be quite **inaccurate**. It will **overestimated** for the correct class and **underestimated** for the incorrect classes. This does become a problem, though, if we’re going to be using the probability estimates themselves—so Naive Bayes should be used with caution for actual decision-making with costs and benefits.\n",
    "\n",
    "\n",
    "2. It perform well in case of **categorical input variables** compared to numerical variable(s). For numerical variable, normal distribution is assumed (bell curve, which is a strong assumption)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applications of Naive Bayes Algorithms\n",
    "\n",
    "- Real time Prediction: Naive Bayes is an eager learning classifier and it is sure fast. Thus, it could be used for making predictions in real time.\n",
    "\n",
    "\n",
    "- Text classification/ Spam Filtering/ Sentiment Analysis: Naive Bayes classifiers mostly used in **text classification** (due to better result in multi-class problems and **independence rule**) have higher success rate as compared to other algorithms. As a result, it is widely used in Spam filtering (identify spam e-mail) and Sentiment Analysis (in social media analysis, to identify positive and negative customer sentiments)\n",
    "\n",
    "\n",
    "- Recommendation System: **Naive Bayes Classifier and Collaborative Filtering** together builds a Recommendation System that uses machine learning and data mining techniques to filter unseen information and predict whether a user would like a given resource or not"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "\n",
    "data = datasets.load_iris()\n",
    "X = data.data  # we only take the first two features.\n",
    "y = data.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: ['1', '2', '3'], 1: ['M', 'S', 'L']}\n",
      "0.3333333333333333\n",
      "0.05555555555555555\n",
      "0.3333333333333333\n",
      "0.14814814814814814\n",
      "the output label of input_data is:-1\n"
     ]
    }
   ],
   "source": [
    "def naive_bayes_classifiers(features, label, input_data):\n",
    "    ## 1. Calculate prior probability of each class\n",
    "    label_category = list(set(label))\n",
    "    label_number = len(label_category)\n",
    "    label_prob = {}\n",
    "    for i in range(0, label_number):\n",
    "        # Laplace smoothing\n",
    "        label_prob[label_category[i]] = len(label[label==label_category[i]] + 1)/len(label) + len(label) * 1\n",
    "    # print(label_prob) #{1: 0.6, -1: 0.4}\n",
    "\n",
    "    \n",
    "    ## 2. Calculate the conditional probability of each sample given their current class\n",
    "    features_set = {}\n",
    "    \n",
    "    ## 2.1 Gather all feature set\n",
    "    for i in range(len(features)):\n",
    "        features_set[i] = list(set(features[i]))\n",
    "    print(features_set)\n",
    "    \n",
    "    ## 2.2 Calculate the probability of each feature under different labels\n",
    "    ## P(Y=label_1)*p(X=feature_1|Y=label_1)*P(X=feature_2|Y=feature_2)\n",
    "    ## e.g. P(Y=1)P(X=2∣Y=1)P(X=1∣Y=1) & P(Y=−1)P(X=2∣Y=−1)P(X=1∣Y=−1) \n",
    "\n",
    "    features_label_prob = {}\n",
    "    \n",
    "    for i in range(len(features)):\n",
    "        for j in range(len(features_set[i])):\n",
    "            for k in range(label_number):\n",
    "                \n",
    "                label_index = np.where(label==label_category[k])\n",
    "                object_feature = np.array(features[i])[label_index]\n",
    "                # print(object_feature)\n",
    "                \n",
    "                # Count the number of sample with a certain feature value in the class and add laplace smoothing\n",
    "                nominator_value = object_feature[object_feature==features_set[i][j]]\n",
    "                nominator = len(nominator_value) + 1\n",
    "                # print(nominator)\n",
    "                \n",
    "                # Count the number of sample in the class and add laplace smoothing\n",
    "                label_select = label[label==label_category[k]]\n",
    "                denominator = len(label_select) + len(features_set[i]) * 1\n",
    "                # print(denominator)\n",
    "                \n",
    "                features_label_prob_key = str(i)+str(features_set[i][j])+str(label_category[k])\n",
    "                features_label_prob[features_label_prob_key] = nominator/denominator\n",
    "                \n",
    "                # print(features_label_prob_key) \n",
    "                # print(denominator)\n",
    "    \n",
    "    # Predict the class given input data\n",
    "    calc_label_prob = {}\n",
    "    \n",
    "    # Given the init product\n",
    "    product = 1\n",
    "    \n",
    "    # Calculate the conditional probability under each class\n",
    "    for i in range(0, label_number):\n",
    "        product = 1\n",
    "        calc_label_prob[label_category[i]] = label_prob[label_category[i]]\n",
    "        # print(calc_label_prob[label_category[i]])\n",
    "        \n",
    "        for j in range(0, len(input_data)):\n",
    "            \n",
    "            # Construct key\n",
    "            key = str(j)+str(input_data[j])+str(label_category[i])\n",
    "            # print(key)\n",
    "            # print(features_label_prob[key])\n",
    "            \n",
    "            product = product * features_label_prob[key]\n",
    "            print(product)\n",
    "            \n",
    "        calc_label_prob[label_category[i]] = calc_label_prob[label_category[i]]*product\n",
    "        \n",
    "\n",
    "    # print(calc_label_prob) #{1: 0.02222222222222222, -1: 0.06666666666666667}\n",
    "\n",
    "    # Return the class with highest probability\n",
    "    output_label = max(calc_label_prob,key=calc_label_prob.get)\n",
    "    \n",
    "    # print(output_label)\n",
    "    return output_label\n",
    "\n",
    "\n",
    "features = [[1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3], \n",
    "            ['S', 'M', 'M', 'S', 'S', 'S', 'M', 'M', 'L', 'L', 'L', 'M', 'M', 'L', 'L']]\n",
    "features = np.array(features)\n",
    "label = [-1, -1, 1, 1, -1, -1, -1, 1, 1, 1, 1, 1, 1, 1, -1]\n",
    "label = np.array(label)\n",
    "input_data = np.array([2, 'S'])\n",
    "output_label = naive_bayes_classifiers(features, label, input_data)\n",
    "print(\"the output label of input_data is:{}\".format(output_label))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "287.796875px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
