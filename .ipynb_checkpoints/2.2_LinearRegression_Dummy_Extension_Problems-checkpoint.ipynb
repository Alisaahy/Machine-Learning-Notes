{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Qualitative Predictors (Dummy)\n",
    "\n",
    "## Predictors with Only Two Levels\n",
    "If a qualitative predictor (also known as a **factor**) only has two **levels**, then we can create a **dummy variable** that takes on two possible numerical values.\n",
    "\n",
    "<img src=\"images/10.png\" width=\"500\">\n",
    "and use this variable as a predictor in the regression equation. This results in the model\n",
    "\n",
    "<img src=\"images/11.png\" width=\"500\">\n",
    "\n",
    "Now β0 can be interpreted as the average credit card balance among males, β0 + β1 as the average credit card balance among females, and β1 is the average difference in credit card balance between females and males.\n",
    "\n",
    "Alternatively, instead of a 0/1 coding scheme, we could create a dummy variable\n",
    "\n",
    "<img src=\"images/12.png\" width=\"500\">\n",
    "and use this variable in the regression equation. This results in the model\n",
    "\n",
    "<img src=\"images/13.png\" width=\"500\">\n",
    "\n",
    "Now β0 can be interpreted as the overall average credit card balance (ignoring the gender effect), and β1 is the amount that females are above the average and males are below the average.\n",
    "\n",
    "**The final predictions for the credit balances of males and females will be identical regardless of the coding scheme used. The only difference is in the way that the coefficient are interpreted.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qualitative Predictors with More than Two Levels\n",
    "\n",
    "When a qualitative predictor has more than two levels, we can create additional dummy variables. For example, for the ethnicity variable we create two dummy variables. The first could be\n",
    "\n",
    "<img src=\"./images/10.png\" width=\"500\">\n",
    "and the second could be\n",
    "<img src=\"./images/11.png\" width=\"500\">\n",
    "\n",
    "Then both of these variables can be used in the regression equation, in order to obtain the model\n",
    "\n",
    "<img src=\"./images/12.png\" width=\"500\">\n",
    "\n",
    "**Baseline**\n",
    "\n",
    "- There will always be **one fewer** dummy variable than the number of levels. The level with no dummy variable—African American in this example—is known as the baseline.\n",
    "\n",
    "<img src=\"images/14.png\" width=\"500\">\n",
    "\n",
    "The p-values associated with the coefficient estimates for the two dummy variables are very large, suggesting no statistical evidence of a real difference in credit card balance between the ethnicities\n",
    "\n",
    "> However, the coefficients and their p-values do depend on the choice of dummy\n",
    "variable coding\n",
    "\n",
    "Rather than rely on the individual coefficients, we can use an **F-test** to test H0 : β1 = β2 = 0; the results of F-test does not depend on the how we code the dummy variable.\n",
    "\n",
    "If the p-value for the F-test is small, it indicates that we cannot reject the null hypothesis that there is no relationship between balance and ethnicity. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extensions of the Linear Model\n",
    "\n",
    "Two of the most important assumptions state that the relationship between the predictors and response are **additive** and **linear**. \n",
    "- **Additive**: the effect of changes in a predictor $X_j$ on the response $Y$ is independent of the values of the other predictors\n",
    "- **Linear**: the change in the response $Y$ due to a one-unit change in $X_j$ is constant, regardless of the value of $X_j$\n",
    "\n",
    "**Here are some common classical approaches for extending the linear model.**\n",
    "\n",
    "## Removing the Additive Assumption by Considering Synergy Effect\n",
    "Consider the standard linear regression model with two variables,\n",
    "\\begin{align}\n",
    "Y = β_0 + β_1X_1 + β_2X_2 + \\epsilon\n",
    "\\end{align}\n",
    "\n",
    "One way of extending this model to allow for interaction effects is to include a third predictor, called an\n",
    "**interaction term**:\n",
    "\n",
    "\\begin{align}\n",
    "Y = β_0 + β_1X_1 + β_2X_2 +  β_3X_1X_2 + \\epsilon \n",
    "\\end{align}\n",
    "\n",
    "**How does inclusion of this interaction term relax the additive assumption?**\n",
    "\n",
    "The model above could be written as:\n",
    "\\begin{align}\n",
    "Y &= β_0 + (β_1+β_3X_2)X_1 + β_2X_2 + \\epsilon  \\\\\n",
    "&= β_0 + \\tilde{β}_1X_1 + β_2X_2 + \\epsilon\n",
    "\\end{align}\n",
    "\n",
    "Since $\\tilde{β}_1$ changes with $X_2$, the effect of $X_1$ on $Y$ is no longer constant: adjusting $X_2$ will change the impact of $X_1$ on $Y$.\n",
    "\n",
    "<img src=\"images/15.png\" width=\"900\">\n",
    "\n",
    "Like the above case:\n",
    "- Sometimes the case that an interaction term has a very small p-value, but the associated main effects (in this case, TV and radio) do not. \n",
    "- According to **hierarchical principle**, if we include an interaction in a model, we should also include the **main effects**, even if the p-values associated with their coefficients are not significant. (If the interaction between X1 and X2 seems important, we should include both X1 and X2 in the model even if their coefficient estimates have large p-values)\n",
    "\n",
    "**Concept of interactions applies on qualitative variables**\n",
    "In the absence of an interaction term, the model takes the form:\n",
    "<img src=\"images/16.png\" width=\"800\">\n",
    "\n",
    "> In this case, the model have 2 parallel lines, one for students and one for non-students, which means that the average effect on balance of a one-unit increase in income does not depend on whether or not the individual is a student. This represents a potentially serious limiatation of the model, since in fact a change in income may have a very different effect on the balance of a student versus a non-student.\n",
    "\n",
    "Adding an interaction variable of income and the dummy variable student, model now becomes:\n",
    "<img src=\"images/17.png\" width=\"850\">\n",
    "\n",
    "Now those regression lines have different intercepts, as well as different slopes. This allows for the possibility that changes in income may affect the credit card balances of students and non-students differently.\n",
    "<img src=\"images/18.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-linear Relationships\n",
    "\n",
    "One simple way of extenting the linear model to accommodate non-linear relationships is known as **polynomial regression**, from which we can include polynomial functions of predictors in the regression model.\n",
    "\n",
    "<img src=\"images/19.png\" width=\"500\">\n",
    "\n",
    "We can see the relationship between mpg and horsepower is in fact non-linear: the data suggest a curved relationship. A simple approach for incorporating non-linear associations in a linear model is to include transformed versions of the predictors in the model. For example, the points in Figure 3.8 seem to have a quadratic shape, suggesting that the model below may provide a better fit:\n",
    "\n",
    "<img src=\"images/20.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Potential Problems of Linear Regrssion\n",
    "\n",
    "## Non-linearity of the Data\n",
    "**Assumption**: The linear regression model assumes that there is a straight-line relationship\n",
    "between the predictors and the response.\n",
    "\n",
    "**How to soleve:**\n",
    "\n",
    "**Residual plots** are a useful graphical tool for identifying non-linearity\n",
    "- Given a simple linear regression model, we can plot the residuals, $e_i = y_i-\\hat{y_i}$ versus the predictor $x_i$. In the case of multiple regression model, since there are multiple predictors, we instead plot the residuals versus the predicted values $\\hat{y_i}$.\n",
    "\n",
    "<img src=\"images/21.png\" width=\"600\">\n",
    "\n",
    "- Ideally, the residual plot will show no discernible pattern.\n",
    "- If the residual plot indicates non-linear associations in the data, then a simple approach is to use **non-linear transformations** of the predictors, such as $\\log{X},\\sqrt{X}, X^2$, in the regression model. \n",
    "> Like the left panel of Figure 3.9, the residual exhibit a clear U-shape, which provides a strong indication of non-linearity in the data. \n",
    "In contrast, the right-hand panel displays the resicual plot of the model containing a quadratic term, and there appears to be little pattern in the residual, suggesting that the quadratic term improves the fit to the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation of Error Terms \n",
    "\n",
    "**Assumption**: The error terms, $\\epsilon_1,\\epsilon_2,...,\\epsilon_n$ of linear regression are uncorrelated.\n",
    "-  If the errors are uncorrelated, then the fact that $\\epsilon_i$ is positive provides little or no information about the sign of $\\epsilon_i+1$.\n",
    "-  If the error terms are correlated, we may have an unwarranted sense of confidence in our model\n",
    " - **estimated standard errors for the regression coefficients** will underestimate the true standard error\n",
    " - **confidence and prediction intervals** will be narrower than they should be. For example, a 95% confidence interval may in reality have a much lower probability than 0.95 of containing the true value of the parameter\n",
    " - **p-values associated with the model** will be lower than they should be\n",
    " - **Lead to erroneously conclude that a parameter is statistically significant**\n",
    "\n",
    "\\begin{align}\n",
    "\\operatorname{Var}\\left(\\sum_{i=1}^n X_i\\right) = \\sum_{i=1}^n \\sum_{j=1}^n \\operatorname{Cov}(X_i, X_j) = \\sum_{i=1}^n \\operatorname{Var}(X_i) + 2\\sum_{1\\le i<j\\le n}\\operatorname{Cov}(X_i,X_j)\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "**Why might correlations among the error terms occur?**\n",
    "- Such correlations frequently occur in the context of **time series** data\n",
    "- In many cases, observations that are obtained at adjacent time points will have **positively correlated errors**. \n",
    "\n",
    "**How to solve:**\n",
    "- We can **plot the residuals** from our model **as a function of time** to identify this correlation. \n",
    "- Ideally, the residual plot will show no discernible pattern.\n",
    "- If the error terms are positively correlated, then we might see **tracking** in the residuals—that is, adjacent residuals may have similar values.\n",
    "\n",
    "<img src=\"images/22.png\" width=\"600\">\n",
    "\n",
    "- In the top panel, there's no evidence of a time-related trend in the residuals.\n",
    "- In the bottom panel, there's a clear pattern in the residuals - adjacent residuals tend to take on similar values.\n",
    "- The center panel illustrates a more moderate case in which the residuals had a correlation of 0.5. The pattern is less clear.\n",
    "\n",
    "**Good experimental design is crucial to mitigate the risk of such correlations**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-constant Variance of Error Terms\n",
    "\n",
    "**Assumption**: the error terms have a constant variance, $Var(\\epsilon_i) = σ^2$.\n",
    "- The standard errors,\n",
    "confidence intervals, and hypothesis tests associated with the linear model\n",
    "rely upon this assumption.\n",
    "\n",
    "But it's often the case that the variances of the error terms are non-constant. \n",
    "- For instance, the variances of the error terms may increase with the value of the response. \n",
    "- One can identify non-constant variances in the errors, or **heteroscedasticity**,from the presence of a funnel shape in residual plot.\n",
    "\n",
    "**How to solve**: \n",
    "\n",
    "Transform the response Y using a concave function such as $\\log{Y}$ or $\\sqrt{Y}$ . Such\n",
    "a transformation results in a greater amount of shrinkage of the larger responses,\n",
    "leading to a reduction in **heteroscedasticity**. The residuals now appear to have constant variance, though there's some evidence of slight non-linear relationship in the data.\n",
    "\n",
    "<img src=\"images/23.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outliers \n",
    "\n",
    "An **Outlier**: is a point for which $y_i$ is far from the value predicted by the model. Outliers can arise for a variety of reasons, such as incorrect recording of an observation during data collection.\n",
    "\n",
    "**Problems of Outlier**:\n",
    "- Effect on the least squares fit line itself,\n",
    "- Effect on interpretation of the model fit\n",
    " - For instance, in this example, the RSE is 1.09 when the outlier is included in the regression, but it is only 0.77 when the outlier is removed.\n",
    "\n",
    "**How to solve:**\n",
    "\n",
    "**Residual Plots** can be used to identify outliers. In practice, it can be difficult to decide how large a residual needs to be before we consider the point to be an outlier. To address this problem, we can plot **studentized residuals**, computed by dividing each residual $e_i$ by its estimated standard error. Observations whose studentized residuals are greater than 3 in absolute value are possible outliers.\n",
    "\n",
    "<img src=\"images/24.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## High Leverage Points \n",
    "\n",
    "**High Leverage**: Observations with high leverage have an unusual value for ${x_i}$. For example, observations 41 in the left-hand panel of Figure 3.13 has high (an unusual value for ${x_i}$), in that the predictor value for this observation is large relative to the other observations. \n",
    "\n",
    "Comparing the left-hand panels of Figure 3.13, we observe that removing the high leverage observation has a much more substantial impact on the least squares line than removing the outlier. In fact, high leverage observations tend to have a sizable impact on the estimated regression line. For this reason, it is important to identify high leverage observations.\n",
    "\n",
    "<img src=\"images/25.png\" width=\"600\">\n",
    "\n",
    "**How to solve:**\n",
    "\n",
    "In multiple linear regression with many predictions, it's possible to have an observation that is well within the range of each individual predictor's values, but that is unusual in terms of the full set of predictions. An example is shown in the center panel. The red observation is well outside of the blue ellipse, but neither its value for X1 nor its value for X2 is unusual. So if we examine just X1 or just X2, we will fail to notice this high leverage point.\n",
    "\n",
    "In order to quantify an observation's leverage, we compute the **leverage statistic**. A large value of this statistic indicates an observation with high leverage. For simple linear regression,\n",
    "\n",
    "\\begin{align}\n",
    "h_i=\\frac{1}{n}+\\frac{(x_i-\\bar{x})^2}{\\sum_{i^{'}=1}^n (x_{i^{'}}-\\bar{x})^2}\n",
    "\\end{align}\n",
    "\n",
    "- $h_i$ increases with the distance of $x_i$ from $\\bar{x}$.\n",
    "- $h_i$ is always between 1/n and 1, and the **average leverage** for all the observations is always equal to $(p+1)/n$ (p is the number of parameters (regression coefficients including the intercept)).\n",
    "- **High leverage**: a leverage statistic that greatly exceeds $(p+1)/n$, high leverage.\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"images/26.png\" width=\"600\">\n",
    "\n",
    "> The right-hand panel of Figure 3.13 provides a plot of the studentized residuals versus $h_i$ for the data in the left-hand panel of Figure 3.13. Observation 41 stands out as having a very high leverage statistic as well as a high studentized residual. In other words, it is an outlier as well as a high leverage observation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collinearity \n",
    "\n",
    "Collinearity: situation in which two or more predictor variables are closely related to one another.\n",
    "\n",
    "**Problems of Collinearity**\n",
    "\n",
    "- Difficult to separate out the individual effects of collinear variables on the response. In other words, since those predictors tend to increase or decrease together, it can be hard to determine how each one separately is associated with the response.\n",
    "- Results in a great deal of uncertainty in the coefficient estimates. \n",
    "- Since collinearity reduces the accuracy of the estimates of the regression coefficients, it causes the standard error for $\\hat{β_j}$ (coefficients) to grow\n",
    " - Recall that the t-statistic for each predictor is calculated by dividing $\\hat{β_j}$ by its standard error. Consequently, collinearity results in a decline in the t-statistic. As a result, in the presence of collinearity, we may fail to reject H0 : βj = 0. This means that the **power** of the hypothesis test—the probability of correctly detecting a non-zero coefficient—is reduced by collinearity.\n",
    "\n",
    "<img src=\"images/27.png\" width=\"600\">\n",
    "\n",
    "**Detection of Collinearity**\n",
    "\n",
    "- **Correlation matrix** of the predictors.\n",
    " - An element of this matrix that is large in absolute value indicates a pair of highly correlated variables.\n",
    " - **Multicollinearity**: it is possible for collinearity to exist between three or more variables even if no pair of variables has a particularly high correlation. We call this multicollinearity.\n",
    " \n",
    "- **Variance Inflation Factor (VIF)**\n",
    " - The VIF is the ratio of the variance of $\\hat{β_j}$ when fitting the full model divided by the variance of $\\hat{β_j}$ if fit on its own. The smallest possible value for VIF is 1, which indicates the complete absence of collinearity. Typically in practice, there's a small amount of collinearity among the predictors.\n",
    " - A VIF value that exceeds 5 or 10 indicates a problematic amount of collinearity. \n",
    " \n",
    " The VIF for each variable can be computed using:\n",
    "\\begin{align}\n",
    "VIF(\\hat{β_j})=\\frac{1}{1-R^2_{X_j|X_{-j}}}\n",
    "\\end{align}\n",
    "\n",
    "where $R^2_{X_j|X_{-j}}$ is the $R^2$ from a regression of $X_j$ onto all of the other predictors. If $R^2_{X_j|X_{-j}}$ is close to one, then collinearity is present, and so the VIF will be large.\n",
    "\n",
    "**Solution to Collinearity**\n",
    "\n",
    "- Drop one of the problematic variables from the regression.\n",
    "- Combine the collinear variables together into a single predictor\n",
    " - E.g.: take the average of standardized versions of limit and rating in order to create a new variable that measures credit worthiness."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
