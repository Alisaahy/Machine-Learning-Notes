{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Directly estimate the test error using validation or cross-validation \n",
    "\n",
    "We can also directly estimate the test error using the validation set and cross-validation method. We can compute the validation set error or the cross-validation error for each model under consideration, and then select the model for which the resulting estimated test error is smallest. \n",
    "\n",
    "## MSE\n",
    "The Mean Squared Error (MSE) is the mean of RSS\n",
    "\n",
    "\\begin{align}\n",
    "RSS&=\\sum_{i=1}^n(y_i-\\hat{y}_i)^2 \\\\\n",
    "&=\\sum_{i=1}^n(y_i-\\hat{\\beta_0}-\\hat{\\beta_1}x_{i1}-,,,-\\hat{\\beta_p}x_{ip})^2\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "## RMSE\n",
    "The Root Mean Squared Error (RMSE) is the square root of MSE\n",
    "\n",
    "## RSE\n",
    "The Residual Standard Error (RSE) is the square root of RSS/degrees of freedom\n",
    "\n",
    "\\begin{align}\n",
    "RSE=\\sqrt{\\frac{RSS}{n-p-1}}\n",
    "\\end{align}\n",
    "\n",
    "Models with more variables can have higher RSE if the decrease in RSS is small relative to the increase in p.\n",
    "\n",
    "\n",
    "\n",
    "**Advantage over statistics evaluation metrics: $C_p, AIC, BIC$, and Adjusted $R^2$**: \n",
    "- Direct estimate of the test error, and makes fewer assumptions about the true underlying model. \n",
    "- Used in a wider range of model selection tasks, even in cases where it is hard to pinpoint the model degrees of freedom or hard to estimate the error variance Ïƒ2.\n",
    "\n",
    "**One-standard-error rule**: We first calculate the standard error of the estimated test MSE for each model size, and then select the smallest model for which the estimated test error is within one standard error of the lowest point on the curve.\n",
    " - **Rationale**: if a set of models appear to **be more or less equally good**, then we might as well choose the **simplest model**â€”that is, the model with the smallest number of predictors. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics for classification model\n",
    "\n",
    "1. Training error rates will usually be lower than test error rates. The reason is that we specifically adjust the parameters of our model to do well on the training data. **The higher the ratio of parameters p to number of samples n, the more we expect this overfitting to play a role.**\n",
    "\n",
    "2. Null accuracy is accuracy that could be achieved by always predicting the most frequent class. Calculating **null accuracy** is a good way to know the minimum we should achieve with our models. In this case, since only 3.33% of the individuals in the training sample defaulted, a simple but useless classifier that always predicts that each individual will not default, regardless of his or her credit card balance and student status, will result in an error rate of 3.33%. Therefore, the trivial **null classifier** will achieve an error rate that is only a bit higher than the LDA training set error rate. This shows how LDA classification accuracy is not that good as it's close to a dumb model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two Types of Error, Confusion Matrix\n",
    "\n",
    "\n",
    "**Confusion Matrix**\n",
    "\n",
    "<img src=\"./images/55.png\" width=600>\n",
    "\n",
    "**Prediction results**ï¼šThe matrix table reveals that LDA predicted that a total of 104 people would default. Of these people, 81 actually defaulted and 23 did not. \n",
    "\n",
    "1. **False positive/Type I Error**: **A test result which incorrectly indicates that a particular condition or attribute is present.** In this case, it can incorrectly assign an individual who does not default to the default category.\n",
    "> Only 23 out of 9,667 of the individuals who did not default were incorrectly labeled. This looks like a pretty low error rate! \n",
    "\n",
    "2. **False negative/Type II Error**ï¼š**A test result that incorrectly indicates that a condition does not hold, while in fact it does.** In this case, it can incorrectly assign an individual who defaults to the no default category. \n",
    "> Of the 333 individuals who defaulted, 252 (or 75.7%) were missed by LDA. So while the overall error rate is low, the error rate among individuals who defaulted is very high. **From the perspective of a credit card company** that is trying to identify high-risk individuals, an error rate of 75.7% among individuals who default may well be unacceptable.\n",
    "\n",
    "\n",
    "Class-specific performance evaluation terms:\n",
    "\n",
    "1. **Sensitivity/TPR**: TP/(TP+FN) = 1-FNR, the percentage of true defaulters that are identified, a low 24.3% in this case.\n",
    "For an **imbalanced dataset**, if we care about the positive outcome and the positive class is also the minority class, sensitivity will be a very important metric to consider.\n",
    "\n",
    "2. **Specificity/TNR**: TN/(TN+FP) = 1-FPR, the percentage of non-defaulters that are correctly identified, here (1 âˆ’ 23/9667)Ã—100 = 99.8%.\n",
    "\n",
    "<img src=\"./images/56.png\" width=1000>\n",
    "<img src=\"./images/57.png\" width=400>\n",
    "<img src=\"./images/58.png\" width=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modifying LDA / Logistic by lowering this threshold of LDA\n",
    "\n",
    "The LDA classifier will yield the **smallest possible total number of misclassified observations, regardless of which class the errors come from**. Therefore, the LDA classifier works by assigning an observation to the class for which the posterior probability pk(X) is greatest. In the two-class case, this amounts to **assigning an observation to the default class if \n",
    "\n",
    "$Pr(default = Yes|X = x)>0.5$**. In other words, by default, the Bayes classifier, and by extension LDA, uses a **threshold of 50%** for the posterior probability of default.\n",
    "\n",
    "However, a credit card company might particularly wish to **avoid incorrectly classifying an individual** who will default, whereas incorrectly classifying an individual who will not default is less problematic. In such case, we are more concerned about incorrectly predicting the default status for individuals who default, which is the **false negative rate**, then we can consider lowering this threshold. In other words, instead of using the 50% threshold, we could instead assign an observation to this class if\n",
    "\n",
    "$Pr(default = Yes|X = x)>0.2$, which means we can label any customer with a **posterior probability** of default **above 20%** to the **default class**.\n",
    "\n",
    "The trade-off that results from modifying the thresh- old value for the posterior probability of default.\n",
    "\n",
    "<img src=\"./images/59.png\" width=700>\n",
    "\n",
    "- Using a threshold of 0.5 minimizes the overall error rate, shown as a black solid line.\n",
    "- But when a threshold of 0.5 is used, the error rate among the individuals who default is quite high (blue dashed line). \n",
    "- As the threshold is reduced, the error rate among individuals who default decreases steadily, but the error rate among the individuals who do not default increases. \n",
    "\n",
    "**How can we decide which threshold value is best? Such a decision must be based on domain knowledge, such as detailed information about the costs associated with default.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC Curve & AUC\n",
    "\n",
    "- ROC curve: a popular graphic for simultaneously displaying the two types of errors for all possible thresholds. \n",
    "- AUC: The overall performance of a classifier, summarized over all possible thresholds, is given by the area under the ROC curve (AUC). \n",
    "\n",
    "<img src=\"./images/60.png\" width=700>\n",
    "\n",
    "- An ideal ROC curve will hug the top left corner, so the larger the AUC the better the classifier. We expect a classifier that performs no better than chance to have an AUC of 0.5\n",
    "\n",
    "\n",
    "- ROC curves are useful for **comparing different classifiers**, since they take into account all possible thresholds.\n",
    " - The choice of a threshold **depends on the importance of TPR and FPR** classification problem.\n",
    " - If there is no external concern about low TPR or high FPR, one option is to weight them equally by choosing the threshold that maximizes ð‘‡ð‘ƒð‘…âˆ’ð¹ð‘ƒð‘…, which the point closest to the **top left corner** of the ROC space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metric to estimate test error regression model\n",
    "\n",
    "The training error can be a poor estimate of the test error. Therefore, RSS, MSE(RSS/n) and $R^2$ are not suitable for selecting the best model among a collection of models with different numbers of predictors.\n",
    "\n",
    "**2 Methods**:\n",
    "\n",
    "1. **Indirectly** estimate test error by making an **adjustment to the training error** to account for the bias due to overfitting.\n",
    "\n",
    "2. **Directly** estimate the test error, using either a validation set approach or a cross-validation approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistics evaluation metric, which adjust training error for the bias: $C_p$, $AIC$, $BIC$, Adjusted $R^2$\n",
    "which indirectly estimate test error by making an adjustment to the training error\n",
    "\n",
    "**Why adjusting training error?**\n",
    "- The training set error is generally an underestimate of the test error. When we achieve a model with minimum training error, it doesn't guarantee that the test error will also be the smallest.\n",
    "- Especially the training error will decrease as more variables are included in the model, but the test error may not. \n",
    "- Therefore, training set RSS and training set $R^2$ cannot be used for model selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mallows' $C_p$\n",
    "\n",
    "For a fitted least squares model containing d predictors, $C_p$ estimate of test MSE:\n",
    "\n",
    "\\begin{align}\n",
    "C_p= \\frac{RSS}{\\hat{\\sigma}^2}+2dâˆ’n = \\frac{1}{n}(RSS+2d\\hat{\\sigma}^2)\n",
    "\\end{align}\n",
    "\n",
    "where $\\hat{\\sigma}^2$ is an estimate of the variance of the error $\\epsilon$\n",
    "\n",
    "**Note**:\n",
    "- The $C_p$ statistic adds a **penalty** of $2d\\hat{\\sigma}^2$ to the training RSS in order to adjust for the fact that the training error tends to underestimate the test error.\n",
    "- The **penalty increases as the number of predictors in the model increases**; this is intended to adjust\n",
    "for the corresponding decrease in training RSS.\n",
    "- If $\\hat{\\sigma}^2$ is an unbiased estimate of $\\sigma^2$, then $C_p$ is an unbiased estimate of test **MSE**. Typically , we estimate $Ïƒ^2$ using $MSE_{all}$, the mean squared error obtained from fitting the model containing **all of the candidate predictors**.\n",
    "\n",
    "**How to determine which set of models is best with $C_p$ statistic?**\n",
    "1. Choose the model with **the lowest $C_p$ value**.\n",
    "2. Identify the model for which the $C_p$ value is **near d**.\n",
    "> When the $C_p$ value is near d, the bias is small (next to none). When it's much greater than d, the bias is substantial. When it's below d, it is due to sampling error; interpret as no bias\n",
    "3. The full model always yields $C_p$ = d, so don't select the full model based on $C_p$.\n",
    "4. If **all models**, except the full model, yield a large $C_p$ not near d, it suggests some **important predictor(s) are missing**. In this case, we are well-advised to identify the predictors that are missing.\n",
    "5. When more than one model has a small value of $C_p$ value near d, in general, choose **the simpler model(( or the model that meets your research needs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AIC\n",
    "The AIC criterion is defined for a large class of models fit by least square. \n",
    "In this case AIC is given by\n",
    "\n",
    "\\begin{align}\n",
    "AIC=\\frac{1}{n\\hat{\\sigma}^2}(RSS+2d\\hat{\\sigma}^2)\n",
    "\\end{align}\n",
    "\n",
    "For least squares models, Cp and AIC are proportional to each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BIC\n",
    "For the least squares model with d predictors, the BIC is, up to irrelevant constants, given by\n",
    "\n",
    "\\begin{align}\n",
    "BIC=\\frac{1}{n}(RSS+\\log(n)d\\hat{\\sigma}^2)\n",
    "\\end{align}\n",
    "\n",
    "BIC will tend to take on a small value for a model with **a low test error**.\n",
    "Since log(n) > 2 for any n > 7, the BIC statistic generally places a **heavier penalty** on models with many variables, and hence results in the selection of **smaller models** than $C_p$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjusted $R^2$ \n",
    "\n",
    "Recall:\n",
    "\\begin{align}\n",
    "R^2=1 âˆ’ \\frac{RSS}{TSS} = 1-\\frac{RSS}{\\sum(y_i-\\bar{y})^2}\n",
    "\\end{align}\n",
    "\n",
    "**TSS**: total sum of squares for the response\n",
    "\n",
    "\n",
    "**Why not $R^2$?**\n",
    "Since RSS always decreases as more variables are added to the model, the $R^2$ always increases as more variables are added. \n",
    "\n",
    "\n",
    "For a least squares model with d variables, **the adjusted $R^2$** statistic is calculated as\n",
    "\\begin{align}\n",
    "Adjusted  \\, R^2=1 âˆ’ \\frac{RSS/(n-d-1)}{TSS/(n-1)}\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "**How to determine which set of models is best with the adjusted $R^2$**:\n",
    "- **A large value of adjusted $R^2$** indicates a model with a small test error. In theory, the model with the largest adjusted $R^2$ will have **only correct variables and no noise variables**. Maximizing the adjusted $R^2$ is equivalent to minimizing $\\frac{RSS}{nâˆ’dâˆ’1}$, which may increase or decrease due to the presence of d in the\n",
    "denominator.\n",
    "> Why: Once **all of the correct variables** have been included in the model, adding additional *noise* variables will lead to only a **very small decrease in RSS**, such variables will lead to an increase in $\\frac{RSS}{nâˆ’dâˆ’1}$, and hence the adjusted $R^2$. Therefiore, unlike the $R^2$ statistic, the adjusted $R^2$ statistic **pays a price for the inclusion of unnecessary variables** in the model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
