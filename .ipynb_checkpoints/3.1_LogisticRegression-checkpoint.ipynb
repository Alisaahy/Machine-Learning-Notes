{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why Not Linear Regression?\n",
    "\n",
    "**Linear regression is not appropriate in the case of a qualitative response.**\n",
    "\n",
    "**Reason:** \n",
    "- In general, there is no natural way to convert a qualitative response variable with more than two levels into a quantitative response that is ready for linear regression. \n",
    "> For example, if we want to encode three categories as a quantitative response varible, Y,\n",
    "Y = 1, if in the 1st category,\n",
    "Y = 2, if in the 2nd category,\n",
    "Y = 3, if in the 3rd category.\n",
    "This coding implies an ordering on the outcomes, and insisting that the difference between the 1st category and the 2nd category is the same as the difference between the 2nd category and the 3rd category. In practice, there's no particular reason for this. So if we choose another way for encoding, like encode the 2nd category to be 1 and the 1st category to be 2, this will imply a totally different relationship amoung the three conditions. Each of these codings would produce fundamentally different linear models taht would ultimately lead to different sets of predictions.\n",
    "- For binary response case, even if we use linear regression on the dummy variable, some of our estimates might be outside the [0,1] interval, making them hard to interpret as probabilities. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "## The Logistic Model\n",
    "\n",
    "**Logistic regression models the probability that Y belongs to a particular category.**\n",
    "\n",
    "For the Default data, logistic regression models the probability of default.\n",
    "\n",
    "For example, the probability of default given balance can be written as $Pr(default = Yes|balance)$, which we abbreviate $p(balance)$ will range between 0 and 1. Then for any given value of balance, a prediction can be made for default. If $p(balance)$ > 0.5, one might predict default = Yes. If a company wishes to be conservative in predicting individuals who are at risk for default, then they may choose to use a lower threshold, such as $p(balance)$ > 0.1.\n",
    "\n",
    "<img src=\"images/39.png\" width=600>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Logistic Function: Assume the logit of the conditional probability of x is a linear form.\n",
    "\n",
    "**Step 1**: To model the relationship between $P(X) = Pr(Y = k|X)$ and $X$, we must model $p(X)$ using a function that gives outputs between 0 and 1 for all values of X. We use the **logistic function (sigmoid function):**\n",
    "\n",
    "\\begin{align}\n",
    "p(X)=\\frac{e^{\\beta_0+\\beta_1X}}{1+e^{\\beta_0+\\beta_1X}}\n",
    "\\end{align}\n",
    "\n",
    "To fit the model, we use a method called **maximum likelihood**.\n",
    "The logistic function will always produce an S-shaped curve range between 0 and 1, and so we'll obtain a sensible prediction regardless of the value of $X$.\n",
    "\n",
    "------------\n",
    "\n",
    "**Step 2**: After some manipulation, we find that:\n",
    "\n",
    "\\begin{align}\n",
    "odds = \\frac{p(X)}{1-p(X)}=e^{\\beta_0+\\beta_1X}\n",
    "\\end{align}\n",
    "\n",
    "$p(X)$ is the probability that an event X occuring, and $1 - p(X)$ is the probability that the event X not occuring. Therefore, quantity $\\frac{p(X)}{1-p(X)}$ is called the odds, and can take on any value between 0 and ∞.\n",
    "Foe example, on average 1 in 5 people with an odds of 1/4 will default, since $p(X) = 2$ implies an odds of $\\frac{0.2}{1-0.2} = \\frac{1}{4}$. \n",
    "\n",
    "------------\n",
    "\n",
    "**Step 3:** Taking the logarithm of both side, we arrive at the log-odds funtion (logit)\n",
    "\n",
    "\\begin{align}\n",
    "\\log{\\frac{p(X)}{1-p(X)}}=\\beta_0+\\beta_1X\n",
    "\\end{align}\n",
    "\n",
    "We see that the logistic model has a logit that is linear in X. In this logistic regression model, increasing X by one unit changes the log odds by $\\beta_1$, or equivalently it multiplies the odds by $e^{\\beta_1}$\n",
    "\n",
    "- There is not a straight-line relationship between $p(X$) and X,\n",
    "- The rate of change in $p(X)$ per unit change in X depends on the current value of X,\n",
    "\n",
    "But regardless of the value of X, if $\\beta_1$ is positive, then increasing X will be associated with increasing $p(X)$ and if $\\beta_1$ is negative then increasing X will be associated with decreasing $p(X)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimating the Regression Coefficients: Maximum Likelihood\n",
    "\n",
    "The basic intuition behind using maximum likelihood to fit a logistic regression model is:\n",
    "\n",
    "We seek estimates for $\\beta_0$ and $\\beta_1$ such that the predicted probability $\\hat{p}(x_i)$ for each $x$ estimated using the logistic function, corresponds as closely as possible to the true observed values.\n",
    "\n",
    "In other words, we try to find $\\hat{\\beta_0}$ and $\\hat{\\beta_1}$ such that plugging these estimates into the logistic function model for $p(X)$, yields a number close to one for all individuals who experinced the event, and a number close to zero for all individuals who did not.\n",
    "\n",
    "This intuition can be formalized using **likelihood function**:\n",
    "\n",
    "\\begin{align}\n",
    "L(\\beta_0,\\beta_1)=\\prod_{i:y_i=1}p(x_i) \\prod_{n:y_n=0}(1-p(x_n))\n",
    "\\end{align}\n",
    "\n",
    "The estimates $\\hat{\\beta_0}$ and $\\hat{\\beta_1}$ are chosen to maximize this **likelihood function.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Predictions\n",
    "\n",
    "Once the coefficients have been estimated, it is a simple matter to compute the probability of default for any given credit card balance.\n",
    "\n",
    "<img src=\"images/40.png\" width=600>\n",
    "\n",
    "For example, using the estimated coefficient, we predict that the default probability for an individual with a balance of $1,000 is\n",
    "\n",
    "\\begin{align}\n",
    "\\hat{p}(X)=\\frac{e^{\\hat{\\beta_0}+\\hat{\\beta_1}X}}{1+e^{\\hat{\\beta_0}+\\hat{\\beta_1}X}}=\\frac{e^{−10.6513+0.0055×1,000}}{1+e^{−10.6513+0.0055×1,000}}=0.00576\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple Logistic Regression\n",
    "\n",
    "We now consider the problem of predicting a binary response using multiple predictors. The generalized **log-odds (logit)** is:\n",
    "\n",
    "\\begin{align}\n",
    "\\log{\\frac{p(X)}{1-p(X)}}=\\beta_0+\\sum_{i=1}^p\\beta_iX\n",
    "\\end{align}\n",
    "\n",
    "where X = (X1, . . .,Xp) are p predictors\n",
    "\n",
    "**Logistic function:**\n",
    "\n",
    "\\begin{align}\n",
    "p(X)=\\frac{e^{\\beta_0+\\sum_{i=1}^p\\beta_iX}}{1+e^{\\beta_0+\\sum_{i=1}^p\\beta_iX}} \\\\\n",
    "\\frac{p(X)}{1-p(X)}=e^{\\beta_0+\\sum_{i=1}^p\\beta_iX}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpret confounding results\n",
    "\n",
    "<img src=\"images/41.png\" width=600>\n",
    "In the logistic regression model with a single variable, the coefficient associated with the dummy student variable is positive, and the associated p-value is statistically significant. This indicates that students tend to have higher default probabilities than non-student.\n",
    "\n",
    "\n",
    "<img src=\"images/42.png\" width=600>\n",
    "In the logistic regression model with multiple variables, the coefficient for the dummy student variable is negative, indicating that, **for a fixed value of balance and income**, a student is less likely to default than a non-student. \n",
    "\n",
    "\n",
    "<img src=\"images/43.png\" width=600>\n",
    "In the left panel, we see that the student default rate is at or below that of the non-student default rate for every value of balance. But the default rates for students and non-students averaged over all values of balance and income, suggest that the overall student default rate is higher than non-student default rate. Consequently, there's a positive coefficient for student in the singel variable model.\n",
    "\n",
    "**Reason**:The variables *student* and *balance* are correlated. Students tend to hold higher levels of debt, which is in turn associated with higher probability of default. \n",
    "\n",
    "**Intuition**: A student is riskier than a non-student if no information about the student’s credit card balance is available. However, that student is less risky than a non-student with the same credit card balance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient descent\n",
    "\n",
    "To maximize the **likelihood function**, I'll use gradient ascent (a classic in optimization theory) to find the best parameters for our model. \n",
    "\n",
    "**Gradient descent:** if we want to find the minimum point on a function, then the best way to move is in the direction of the gradient. If you continuously take small steps in this direction, you'll eventually make it to a local maximum. In the case of logistic regression you can prove that the result will always be a global maximum.\n",
    "\n",
    "Steps:\n",
    "- Step 1: decide the cost function\n",
    "- Step 2: calculate gradient, which are derivatives for each parameter in the function. For example, the gradient of a function $f(x, y)$ is given by the equation:\n",
    "\n",
    "\\begin{equation}\n",
    "\\nabla{f(x,y)} = \\begin{bmatrix}\n",
    "    \\frac{\\partial f(x, y)}{\\partial x} \\\\\n",
    "    \\frac{\\partial f(x, y)}{\\partial y}\n",
    "\\end{bmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "So this gradient means that we'll move in the x direction by amount of $\\frac{\\partial f(x, y)}{\\partial x}$ and in the y direction $\\frac{\\partial f(x, y)}{\\partial y}$.\n",
    "- Step 3: pick a random value of parameter to begin updating. Each updates takes a step, which is \n",
    " - step_size = slope * learning rate (predetermined)\n",
    " - And the updating X value new_X should be: new_X = old_X - step_size. **For gradient ascent in the logistic case: new_X = old_X + step_size**\n",
    "\n",
    "\n",
    "- Repeat the last step until we reach a stopping condition: either a specific number of steps or the algorithm is within a certain tolerance margin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression for non-linear decision boundary with polynomial functions\n",
    "\n",
    "<img src=\"./images/62.png\" width=600>\n",
    "\n",
    "Ordinary logistic regression function does not have enough flexibility to model such a non-linear decision boundary in this setting. \n",
    "\n",
    "We can easily extend logistic regression to obtain a non-linear decision boundary by using polynomial functions of the predictors, as we did for linear regression. For example, we can fit a quadratic logistic regression model, given by:\n",
    "\n",
    "\\begin{align}\n",
    "\\log{\\frac{p(X)}{1-p(X)}}=\\beta_0+\\beta_1X_1+\\beta_2X_1^2+\\beta_3X_2+\\beta_4X_2^2\n",
    "\\end{align}\n",
    "\n",
    "In practice, for real data, the Bayes decision boundary and the test error rates are unknown. So **how might we decide between models with different flexibility?**\n",
    "\n",
    "<img src=\"./images/63.png\" width=600>\n",
    "\n",
    "Overall, the training error tends to decrease as the flexibility of the fit increases. In contrast, the test error displays a characteristic **U-shape**. The 10-fold CV error rate provides a pretty good approximation to the test error rate. It reaches a minimum when fourth-order polynomials are used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression algorithm\n",
    "\n",
    "Let's assume we have an input data matrix X, and every training input vector is $\\vec{x} = [x_1, x_2, \\dots, x_m]$ and the model's parameters are $\\vec{w} = [\\beta_0, \\beta_1, \\cdots, \\beta_n]$. \n",
    "- As we are keeping the convention of letting ${x_0} = 1$, so that $p(X) = p_w(w^T\\vec{x}) = p_w(\\beta_0 + \\sum_{j=1}^{d}\\beta_jx_j)$\n",
    "- For every $x_i$, $p_w(x_i)$ is only affected by the $\\vec{w}$, which should be $p_w(w^T\\vec{x})$\n",
    "- $p(X)$ takes the sigmoid function\n",
    "\n",
    "**Step 1: decide the cost function**\n",
    "Recall our likelihood function. It can be written as:\n",
    "\n",
    "\\begin{align}\n",
    "L(w)=\\prod_{i:y_i=1}p_w(x_i) \\prod_{i:y_n=0}(1-p_w(x_n)) = \\prod_{n=1}^m(p_w(x_i))^{y_i}(1 - p_w(x_i))^{1-y_i}\n",
    "\\end{align}\n",
    "\n",
    "Take the log of the function, we can get the log likelihood is:\n",
    "\\begin{align}\n",
    "l(w)=\\sum_{1}^{m}(y_i)log({p_w(x_i)}) + (1-y_i)log(1-{p_w(x_i)})\n",
    "\\end{align}\n",
    "\n",
    "**Step 2: calculate gradient**\n",
    "To maximize the likelihood, I'll firstly calculate the gradient $\\nabla_w{l(w)}$, which is the derivative of this function. \n",
    "\n",
    "The gradient$[j]$ for a specific parameter $\\beta_j$, which is an element of $w$, is the derivative with respect to $\\beta_j$:\n",
    "\n",
    "\\begin{equation}\n",
    "\\nabla_w{l(w)} = \\frac{\\partial l(w)}{\\partial \\beta_j} = \\sum_{i=1}^{m}(y_i - {p_w(x_i)})x_i^j\n",
    "\\end{equation}\n",
    "\n",
    "- (${x_i}, {y_i}$) is every training example,\n",
    "- ${p_w(x_i)}$ is the output sigmod result of the input ${x_i}$,\n",
    "- $x_i^j$ is the value of ${x_i}$ corresponding to the parameter $\\beta_j$ (0 < j < n)\n",
    "\n",
    "**Step 3: begin updating**\n",
    "\n",
    "Recall the gradient ascent update function $w := w+\\alpha \\nabla_w{l(w)}$, the update to our parameters $\\beta_j$ that results in each small step can be calculated as:\n",
    "\n",
    "\\begin{align}\n",
    "\\beta_j := \\beta_j + \\alpha\\sum_{i=1}^{m}(y_i - {p_w(x_i)})x_i^j\n",
    "\\end{align}\n",
    "\n",
    "And for $\\vec{w}$:\n",
    "<img src=\"images/44.png\" width=600>\n",
    "\n",
    "Recall that:\n",
    "\\begin{align}\n",
    "p(X)=\\frac{e^{\\beta_0+\\beta_1X}}{1+e^{\\beta_0+\\beta_1X}} = \\frac{1}{1+e^{-(\\beta_0+\\beta_1X)}}\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1.0 / (1 + np.exp(-x))\n",
    "\n",
    "# Part 1: calculate gradient ascent, update weight and get the final weight results\n",
    "def gradient_ascent(X_train, y_train):\n",
    "    \n",
    "    # Add x0 = 1 to every train record\n",
    "    intercept = np.ones((X_train.shape[0], 1))\n",
    "    X_train = np.hstack((intercept, X_train))\n",
    "    \n",
    "    # Convert to NumPy matrix\n",
    "    dataMatrix = np.mat(X_train)\n",
    "    # The np.mat(y_train) will generate a 1*100 row vector\n",
    "    # For the matrix math to work, we need to convert it to a column vector. SO I take the transpose\n",
    "    labelMat = np.mat(y_train).transpose()\n",
    "    # Get the size of the data matrix, compute how many features we have in the dataset\n",
    "    # Set a parameter vector(weight) for gradient ascent algorithm\n",
    "    m, n = np.shape(dataMatrix)\n",
    "    \n",
    "    # Step size\n",
    "    alpha = 0.001\n",
    "    # Stop rule, number of times I'll repeat the calculation before stopping\n",
    "    maxCycles = 1000\n",
    "    weights = np.ones((n, 1))\n",
    "    \n",
    "    # Iterates over the dataset, and finally return the weights\n",
    "    for k in range(maxCycles): \n",
    "        Pw = sigmoid(np.dot(dataMatrix, weights))     # Matrix multiplication\n",
    "        error = (labelMat - Pw)                       # Vector subtraction, Pw is a column vector\n",
    "        weights = weights + alpha * np.dot(dataMatrix.transpose(), error) # Matrix mult\n",
    "    print(weights)\n",
    "    return weights\n",
    "    \n",
    "    \n",
    "# Part 2: classify test data\n",
    "def classifyVector(X_test, weights):\n",
    "\n",
    "    # Add x0 = 1 to every test record and make it to be a matrix\n",
    "    intercept = np.ones((X_test.shape[0], 1))\n",
    "    X_test = np.hstack((intercept, X_test))\n",
    "    testMatrix = np.array(X_test)\n",
    "    \n",
    "    # Use sigmoid function to get predicted probability\n",
    "    prob = sigmoid(np.dot(testMatrix, weights))\n",
    "    \n",
    "    y_pred = []\n",
    "    # Set the threshold to be 0.5, \n",
    "    # which means that if the predicted probability is larger than 0.5, the predicted label will be 1\n",
    "    for i in prob:\n",
    "        if i > 0.5: \n",
    "            y_pred.append(1.0)\n",
    "        else: \n",
    "            y_pred.append(0.0) \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# I'll use an artificially generated dataset to train the model, using sklearn’s make_blobs function. \n",
    "# It can generate clusters of data.\n",
    "\n",
    "from sklearn.datasets import make_blobs\n",
    "from matplotlib import pyplot as plt\n",
    "from pandas import DataFrame\n",
    "# generate 2d classification dataset\n",
    "centers = [[0, 0], [4, 4]]\n",
    "X_train, y_train = make_blobs(n_samples=1000, centers=centers, n_features=2)\n",
    "# scatter plot, dots colored by class value\n",
    "df = DataFrame(dict(x=X_train[:,0], y=X_train[:,1], label=y_train))\n",
    "colors = {0:'red', 1:'blue'}\n",
    "fig, ax = plt.subplots()\n",
    "grouped = df.groupby('label')\n",
    "for key, group in grouped:\n",
    "    group.plot(ax=ax, kind='scatter', x='x', y='y', label=key, color=colors[key])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-7.97532647]\n",
      " [ 2.05981479]\n",
      " [ 1.98377904]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "matrix([[-7.97532647],\n",
       "        [ 2.05981479],\n",
       "        [ 1.98377904]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient_ascent(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-7.97532647]\n",
      " [ 2.05981479]\n",
      " [ 1.98377904]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test, y_test = make_blobs(n_samples=1000, centers=centers, n_features=2)\n",
    "classifyVector(X_test, gradient_ascent(X_train, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
