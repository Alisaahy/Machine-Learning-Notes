{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to estimate $f$\n",
    "\n",
    "**Parametric methods**\n",
    "- Make an assumption about the functional form, or shape, of $f$, like a linear assumption.\n",
    "- After a model has been selected, we use the training data to fit or train the model. For linear regression, the most common approach is (ordinary) least squares.\n",
    "\n",
    "- **Advantages**\n",
    " - Reduces the problem of estimating $f$ down to estimaing a set of parameters.\n",
    " - Like linear models, it allows for simple and interpretable inference.\n",
    "- **Disadvantage** \n",
    " - We should give assumption about the $f(X)$. If the chosen model is too far from the true $f$, and prediction accuracy is our goal, then the estimate will be poor.\n",
    "\n",
    "**Non-parametric methods**\n",
    "- **Advantages**\n",
    " - Don't explicit assume a form for $f$. Thereby they can be flexible to fit a wider range of possible $f$.\n",
    "- **Disadvantage** \n",
    " - Require a large number of observations for an accurate estimate for $f$. \n",
    " \n",
    "The parametric approach will outperform the non-parametric approach if the parametric form that has been selected is close to the true form of $f$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised vs unsupervised\n",
    "- supervised model: for each observation of the predictor measurements, $x_1$ to $x_n$, there's an associated response measurement $y_1$ to $y_n$.\n",
    "- Unsupervised: for every observation i to n, we observe a vector of measurements $x_1$ to $x_n$ but no associated response $y$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The trade-off between prediction accuracy and model interpretability\n",
    "<img src=\"images/1.png\" width=\"500\">\n",
    "\n",
    "- If we are mainly interested inference, we can choose restrictive as they're more interepretable.\n",
    "- If interpretability is not a concern, most flexible models are not necessarily the best because of potential overfit. We can start from a less flexible method as a baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MSE\n",
    "\n",
    "In the regression setting, the most commonly-used measure is the $mean$ $squared$ $error$ (MSE)\n",
    "\n",
    "\\begin{align}\n",
    "MSE&=\\frac{1}{n}*sum_{i=1}^n(y_i-\\hat{f}(x_i))^2 \\\\\n",
    "\\end{align}\n",
    "\n",
    "AS model flexibility increases, training MSE will decrease, but the test MSE may not. When a given method yields a small training MSE but a large test MSE, we are said to be overfitting the data. This happens because our statistical learning procedure is working too hard to find patterns in the training data, and may be picking up some patterns that are just caused by random chance rather than by true properties of the unknown function $f$. \n",
    "\n",
    "But we almost always expect the training MSE to be smaller than the test MSE because most statistical learning methods either directly or indirectly seek to minimize the training MSE. Overfitting refers specifially to the case in which a less flexible model would have tielded a smaller test MSE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Bias-Variance Trade-off\n",
    "\n",
    "The expected test MSE, for a given vaue $x_0$, can aways be decomposed into the sum of three quantities: the $variance$ of $\\hat{f}(x_0)$, the squared $bias$ of $\\hat{f}(x_0)$ and the variance of the error terms $\\epsilon$, which is the irreducible error. That is,\n",
    "\n",
    "\\begin{align}\n",
    "E(y_0-\\hat{f}(x_0))^2&= Var(hat{f}(x_0)) + [Bias(\\hat{f}(x_0))]^2 + Var(\\epsilon)\\\\ \n",
    "\\end{align}\n",
    "\n",
    "Therefore, in order to minimize the expected test MSE, we need to select a model that simultaneously achieves low variance and low bias.\n",
    "\n",
    "- Variance\n",
    "\n",
    " - Variance refers to the amount by which the predicted value would change if we estimated it using a different training data set. It's the degree to which these predictions vary between model iterations.\n",
    " - Ideally, the estimate predicted value should not vary too much between training sets. If a method has high variance, then small changes in the training data can result in large changes in the predicted value. In general, more flexible statistical methods have higher variance.\n",
    " > Mathematically, variance is the squared difference between our long-term expectation for the model's performance, which is the averaged performance over many datasets D, ED[hD(x)], and what we expect in a representative run on a dataset D (hat y)\n",
    " \n",
    " \\begin{align}\n",
    " Variance = E[(h-hat{y})^2]\n",
    " \\end{align}\n",
    "\n",
    "\n",
    "\n",
    "- Bias\n",
    "\n",
    " - Bias occurs due to the simplified assumption made by our models when solving complex real problems. It measures how far off in general these models' predictions are from the correct value.\n",
    " - Generally, more flexible methods result in less bias.\n",
    "  > Mathematically, bias-squared is the squared difference between the true target variable y (or the best possible prediction for x, f(x)), and our “long-term” expectation for what the model will perform if we averaged over many datasets D, ED[hD(x)]\n",
    "  \n",
    " \\begin{align}\n",
    " Bias^2 = E[(f-h)^2]\n",
    " \\end{align}\n",
    " \n",
    " \n",
    " \n",
    "For more flexible methods, the variance will increase and the bias will decrease. The relative rate of change of these two qtities determines whether the test MSE increases or decreases. In general, the bias tends to decrease faster than the variance increase at the begining. But at some point, increasing the flexibility of the model begin to have little impact on the bias but significantly increase the variance.\n",
    "\n",
    "<img src=\"images/2.png\" width=\"500\">\n",
    "<center>So mostly, the test MSE plot will be like a U-shape.</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
