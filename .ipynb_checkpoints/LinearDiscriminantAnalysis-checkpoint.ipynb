{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why Linear Discriminant Analysis\n",
    "\n",
    "1. When the classes are well-separated, the parameter estimates for the logistic regression model are surprisingly unstable. Linear discriminant analysis does not suffer from this problem.\n",
    "\n",
    "2. If n is small and the distribution of the predictors X is approximately normal in each of the classes, the linear discriminant model is again more stable than the logistic regression model.\n",
    "\n",
    "3. Linear discriminant analysis is popular when we have more than two response classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA in a nutshell\n",
    "The LDA classifier results from assuming hat the observations within each class come from a normal distribution with a class-specific mean vector and a common variance $σ^2$, and plugging estimates for these parameters into the Bayes classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Bayes’ Theorem for Classification\n",
    "Suppose that we wish to classify an observation into one of K classes, where K ≥ 2.\n",
    "\n",
    "Let $\\pi_k=Pr(Y=k)$ represent the overall or ***prior*** probability that a randomly chosen observation comes from the kth class; this is the probability that a given observation is associated with the kth category of the response variable Y . \n",
    "\n",
    "Let $f_k(X) ≡ Pr(X = x|Y = k)$ denote the ***density function*** of X for an observation that comes from the kth class. In other words, $f_k(X)$ is relatively large if there is a high probability that an observation in the kth class has X ≈ x.\n",
    "\n",
    "Then, **Bayes’ theorem** states that\n",
    "\n",
    "\\begin{align}\n",
    "P(A\\mid B) = \\frac{P(B \\mid A) P(A)}{P(B)}\n",
    "\\end{align}\n",
    "\n",
    "\\begin{align}\n",
    "Pr(Y=k \\mid X=x)= \\frac{P(X=x \\mid Y=k) P(Y=k)}{P(X=x)} = \\frac{\\pi_k f_k(x)}{\\sum_{l=1}^K\\pi_lf_l(x)} \n",
    "\\end{align}\n",
    "\n",
    "**Posterior**: We will use the abbreviation $p_k(X) = Pr(Y = k|X)$. We reger to this as the **posterior** probability that an observation X = x belongs to the kth class, given the predictor value for that observation.\n",
    "\n",
    "**Estimating $π_k$**: compute the fraction of the training observations that belong to the kth class.\n",
    "\n",
    "**Estimating $f_k(X)$**: more challenging, unless we assume some simple forms for these densities.\n",
    "\n",
    "> - Prior [θ]: The distribution of the param- eters in the absence of any data\n",
    "> - Posterior [θ|Y]: The conditional density of the parameters given the data\n",
    "> - Likelihood [Y|θ]: The conditional density of the data given the parameters.\n",
    "Assume that you know the parameters exactly, what is the distribution of the data\n",
    "> - Posterior = Prior * Likelihood, which is P (A | B) = (P (B | A) * P(A)) / P(B), P(B) acts as a normalized factor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Discriminant Analysis for p = 1\n",
    "\n",
    "Assume p = 1 — that is, we have only one predictor. We would like to obtain an estimate for $f_k(x)$ that we can estimate $p_k(x)$. **We will then classify an observation to the class for which $p_k(x)$ is greatest.**\n",
    "\n",
    "## Assumptions\n",
    "In order to estimate $f_k(x)$, we will first make some assumptions about its form.\n",
    "\n",
    "### Assumption 1\n",
    "Assume that $f_k(x)$ is normal or Gaussian:\n",
    "\\begin{align}\n",
    "f_k(x)=\\frac{1}{\\sqrt{2\\pi}\\sigma_k}\\exp{\\left( -\\frac{1}{2\\sigma_k^2}(x-\\mu_k)^2 \\right)}\n",
    "\\end{align}\n",
    "where $μ_k$ and $σ_k^2$ are the mean and variance parameters for the kth class.\n",
    "\n",
    "### Assumption 2\n",
    "Assume that variances among all classes are equal, $\\sigma_1^2=...=\\sigma_k^2$: that is, there is a shared variance term across all K classes, which for simplicity we can denote by $\\sigma^2$.\n",
    "\n",
    "Plugging into the formular:\n",
    "\\begin{align}\n",
    "p_k(x)=\\frac{\\pi_k \\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp{\\left( -\\frac{1}{2\\sigma^2}(x-\\mu_k)^2 \\right)}}{\\sum_{l=1}^K\\pi_l\\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp{\\left( -\\frac{1}{2\\sigma^2}(x-\\mu_l)^2 \\right)}}\n",
    "\\end{align}\n",
    "\n",
    "### Apply Bayes classifier technique\n",
    "The **Bayes classifier** involves assigning an observation X = x to the class for which $p_k(x)$ is largest. **Taking the log** of $p_k(x)$ and **rearranging** the terms, it is not hard to show that this is equivalent to\n",
    "assigning the observation to the class for which:\n",
    "\n",
    "\\begin{align}\n",
    "\\delta_k(x)=x\\frac{\\mu_k}{\\sigma^2}-\\frac{\\mu_k^2}{2\\sigma^2}+\\log(\\pi_k)\n",
    "\\end{align}\n",
    "\n",
    "is largest.\n",
    "\n",
    "For instance, if K = 2 and π1 = π2, then the Bayes classifier assigns an observation to class 1 if $2xμ_1-μ^2_1 > 2xμ_2-μ^2_2$, which is $2x(μ_1 − μ_2) > μ^2_1 − μ^2_2$, and to class 2 otherwise. In this case, the Bayes decision boundary corresponds to the point where\n",
    "\n",
    "\\begin{align}\n",
    "x=\\frac{\\mu_1^2-\\mu_2^2}{2(\\mu_1-\\mu_2)}=\\frac{\\mu_1+\\mu_2}{2}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters Estimation\n",
    "\n",
    "In practice, even if we are quite certain of our assumption that X is drawn from a Gaussian distribution within each class, we still have to estimate the parameters $μ_1, . . . , μ_K, π_1, . . . , π_K$, and $σ^2$.\n",
    "\n",
    "**The linear discriminant analysis (LDA)** method approximates the Bayes classifier by plugging estimates for $ π_1,..., π_K, μ_1,..., μ_K$, and $σ^2$ into the formular\n",
    "\n",
    "\\begin{align}\n",
    "\\hat{\\mu}_k=\\frac{1}{n_k}\\sum_{i:y_i=k}x_i  \\quad (4.15) \\\\\n",
    "\\hat{\\sigma}^2=\\frac{1}{n-K}\\sum_{k=1}^K\\sum_{i:y_i=k}(x_i-\\hat{\\mu_k})^2 \\quad (4.16)\\\\\n",
    "\\hat{\\pi_k}=\\frac{n_k}{n}\n",
    "\\end{align} \n",
    "\n",
    "where n is the total number of training observations, and $n_k$ is the number of training observations in the kth class. \n",
    "\n",
    "$\\hat{\\mu}_k$: the average of all the training observations from the kth class;\n",
    "$\\hat{\\sigma}^2$: a weighted average of the sample variances for each of the K classes;\n",
    "$\\hat{\\pi_k}$: the proportion of the training observations that belong to the kth class.\n",
    "\n",
    "\n",
    "##  LDA classifier\n",
    "The LDA classifier plugs estimates and assigns an observation X = x to the class for which\n",
    "\n",
    "\\begin{align}\n",
    "\\hat{\\delta}_k(x)=x\\frac{\\hat{\\mu}_k}{\\hat{\\sigma}^2}-\\frac{\\hat{\\mu}_k^2}{2\\hat{\\sigma}^2}+\\log(\\hat{\\pi}_k)\n",
    "\\end{align} \n",
    "\n",
    "is largest.\n",
    "\n",
    "The word ***linear*** in the classifier’s name stems from the fact that the ***discriminant functions*** $\\hat{\\delta}_k(x)$ are linear functions of x.\n",
    "\n",
    "<img src=\"./images/52.png\" width=600>\n",
    "\n",
    "The right-hand panel of Figure 4.4 displays a histogram of a random\n",
    "sample of 20 observations from each class. \n",
    "\n",
    "To implement LDA,\n",
    "\n",
    "1. Estimating $π_k$, $μ_k$, and $σ^2$.\n",
    "2. Compute the decision boundary, shown as a black solid line, that results from assigning an observation to the class for which $\\hat{\\delta}_k(x)$ is largest. All points to the left of this line are assigned to the green class, while points to the right of this line are assigned to the purple class.\n",
    "\n",
    "In this case, since $n_1$ = $n_2$ = 20, we have $\\hat{\\pi_1}$ = $\\hat{\\pi_2}$. As a result, the decision boundary corresponds to the midpoint between the sample means for the two classes, $\\frac{\\mu_1+\\mu_2}{2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Discriminant Analysis for p >1 \n",
    "\n",
    "Assume that X = (X1,X2, . . .,Xp) is drawn from a **multivariate Gaussian** (or multivariate normal) distribution, with a class-specific mean vector and a common covariance matrix.\n",
    "\n",
    "\n",
    "## Multivariate Gaussian Distribution\n",
    "\n",
    "Multivariate Gaussian distribution assumes that each individual predictor follows a one-dimensional normal distribution with some correlation between each pair of predictors.\n",
    "\n",
    "<img src=\"./images/54.png\" width=600>\n",
    "\n",
    "> Above are 2 examples of multivariate Gaussian distributions with p = 2. The height of the surface at any point represents the probability that both $X_1$ and $X_2$ fall in a small region around that point. In either panel, if the sur- face is cut along the $X_1$ axis or along the $X_2$ axis, the resulting cross-section will have the shape of a one-dimensional normal distribution. The left-hand panel illustrates an example in which Var($X_1$) = Var($X_2$) and Cor($X_1$, $X_2$) = 0; this surface has a characteristic bell shape. However, the bell shape will be distorted if the predictors are correlated or have unequal variances, as is illustrated in the right-hand panel. In this situation, the base of the bell will have an elliptical, rather than circular shape.\n",
    "\n",
    "To indicate that a p-dimensional random variable X has a multivariate Gaussian distribution, we write X ∼ N(μ, Σ). Here E(X) = μ is the mean of X (a vector with p components), and Cov(X) = Σ is the p × p **covariance matrix** of X. Formally, the **multivariate Gaussian density** is defined as\n",
    "\n",
    "\\begin{align}\n",
    "f(x)=\\frac{1}{\\sqrt{(2\\pi)^{p}|Σ|}}\\exp{\\left( \\frac{1}{2}(x-\\mu)^TΣ^{-1}(x-\\mu) \\right)}\n",
    "\\end{align} \n",
    "\n",
    "In the case of p > 1 predictors, the **LDA classifier** assumes that the observations in the kth class are drawn from a multivariate Gaussian distribution $N(μ_k, Σ)$, where $μ_k$ is a class-specific mean vector, and Σ is a covariance matrix that is common to all K classes.\n",
    "\n",
    "Plugging the density function for the kth class, $f_k(X = x)$, into the LDA classifier formular and performing a little bit of algebra to assign an observation X = x to the class for which\n",
    "\n",
    "\\begin{align}\n",
    "\\delta_k(x)=x^TΣ^{-1}\\mu_k-\\frac{1}{2}\\mu_k^TΣ^{-1}\\mu_k+\\log{\\pi_k}   \\quad \\quad (4.19)\n",
    "\\end{align} \n",
    "\n",
    "is largest.\n",
    "\n",
    "<img src=\"./images/53.png\" width=600>\n",
    "\n",
    "\n",
    "And the LDA classifier decision boundary represent the set of values x for which ${\\delta}_k(x) = {\\delta}_l(x)$; i.e.\n",
    "\n",
    "\\begin{align}\n",
    "x^TΣ^{-1}\\mu_k-\\frac{1}{2}\\mu_k^TΣ^{-1}\\mu_k = x^TΣ^{-1}\\mu_l-\\frac{1}{2}\\mu_l^TΣ^{-1}\\mu_l\n",
    "\\end{align}\n",
    "for k ≠ l.\n",
    "\n",
    "Once again, we need to estimate the unknown parameters $μ_1, . . . , μ_K, π_1, . . . , π_K$, and Σ; the formulas are similar to those used in the one dimensional case above. To assign a new observation X = x, **LDA** plugs these estimates into (4.19) and classifies to the class for which $\\hat{\\delta}_k(x)$ is largest. Overall, the LDA decision boundaries are pretty close to the Bayes decision boundaries, shown again as dashed lines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Caveats\n",
    "\n",
    "If we perform LDA on the Default data in order to predict whether or not an individual will default on the basis of credit card balance and student status. The LDA model fit to the 10,000 training samples results in a **training error rate of 2.75%**.\n",
    "\n",
    "1. Training error rates will usually be lower than test error rates. The reason is that we specifically adjust the parameters of our model to do well on the training data. **The higher the ratio of parameters p to number of samples n, the more we expect this overfitting to play a role.**\n",
    "\n",
    "2. Null accuracy is accuracy that could be achieved by always predicting the most frequent class. Calculating **null accuracy** is a good way to know the minimum we should achieve with our models. In this case, since only 3.33% of the individuals in the training sample defaulted, a simple but useless classifier that always predicts that each individual will not default, regardless of his or her credit card balance and student status, will result in an error rate of 3.33%. Therefore, the trivial **null classifier** will achieve an error rate that is only a bit higher than the LDA training set error rate. This shows how LDA classification accuracy is not that good as it's close to a dumb model.\n",
    "\n",
    "\n",
    "## Two Types of Error, Confusion Matrix\n",
    "\n",
    "\n",
    "\n",
    "**Confusion Matrix**\n",
    "\n",
    "<img src=\"./images/55.png\" width=600>\n",
    "\n",
    "**Prediction results**：The matrix table reveals that LDA predicted that a total of 104 people would default. Of these people, 81 actually defaulted and 23 did not. \n",
    "\n",
    "1. **False positive/Type I Error**: **A test result which incorrectly indicates that a particular condition or attribute is present.** In this case, it can incorrectly assign an individual who does not default to the default category.\n",
    "> Only 23 out of 9,667 of the individuals who did not default were incorrectly labeled. This looks like a pretty low error rate! \n",
    "\n",
    "2. **False negative/Type II Error**：**A test result that incorrectly indicates that a condition does not hold, while in fact it does.** In this case, it can incorrectly assign an individual who defaults to the no default category. \n",
    "> Of the 333 individuals who defaulted, 252 (or 75.7%) were missed by LDA. So while the overall error rate is low, the error rate among individuals who defaulted is very high. **From the perspective of a credit card company** that is trying to identify high-risk individuals, an error rate of 75.7% among individuals who default may well be unacceptable.\n",
    "\n",
    "\n",
    "Class-specific performance evaluation terms:\n",
    "\n",
    "1. **Sensitivity/TPR**: TP/(TP+FN) = 1-FNR, the percentage of true defaulters that are identified, a low 24.3% in this case.\n",
    "For an **imbalanced dataset**, if we care about the positive outcome and the positive class is also the minority class, sensitivity will be a very important metric to consider.\n",
    "\n",
    "2. **Specificity/TNR**: TN/(TN+FP) = 1-FPR, the percentage of non-defaulters that are correctly identified, here (1 − 23/9667)×100 = 99.8%.\n",
    "\n",
    "<img src=\"./images/56.png\" width=1000>\n",
    "<img src=\"./images/57.png\" width=400>\n",
    "<img src=\"./images/58.png\" width=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modifying LDA by lowering this threshold of LDA\n",
    "\n",
    "The LDA classifier will yield the **smallest possible total number of misclassified observations, regardless of which class the errors come from**. Therefore, the LDA classifier works by assigning an observation to the class for which the posterior probability pk(X) is greatest. In the two-class case, this amounts to **assigning an observation to the default class if \n",
    "\n",
    "$Pr(default = Yes|X = x)>0.5$**. In other words, by default, the Bayes classifier, and by extension LDA, uses a **threshold of 50%** for the posterior probability of default.\n",
    "\n",
    "However, a credit card company might particularly wish to **avoid incorrectly classifying an individual** who will default, whereas incorrectly classifying an individual who will not default is less problematic. In such case, we are more concerned about incorrectly predicting the default status for individuals who default, which is the **false negative rate**, then we can consider lowering this threshold. In other words, instead of using the 50% threshold, we could instead assign an observation to this class if\n",
    "\n",
    "$Pr(default = Yes|X = x)>0.2$, which means we can label any customer with a **posterior probability** of default **above 20%** to the **default class**.\n",
    "\n",
    "The trade-off that results from modifying the thresh- old value for the posterior probability of default.\n",
    "\n",
    "<img src=\"./images/59.png\" width=700>\n",
    "\n",
    "- Using a threshold of 0.5 minimizes the overall error rate, shown as a black solid line.\n",
    "- But when a threshold of 0.5 is used, the error rate among the individuals who default is quite high (blue dashed line). \n",
    "- As the threshold is reduced, the error rate among individuals who default decreases steadily, but the error rate among the individuals who do not default increases. \n",
    "\n",
    "**How can we decide which threshold value is best? Such a decision must be based on domain knowledge, such as detailed information about the costs associated with default.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC Curve & AUC\n",
    "\n",
    "- ROC curve: a popular graphic for simultaneously displaying the two types of errors for all possible thresholds. \n",
    "- AUC: The overall performance of a classifier, sum- marized over all possible thresholds, is given by the area under the ROC curve (AUC). \n",
    "\n",
    "<img src=\"./images/60.png\" width=700>\n",
    "\n",
    "- An ideal ROC curve will hug the top left corner, so the larger the AUC the better the classifier. We expect a classifier that performs no better than chance to have an AUC of 0.5\n",
    "\n",
    "\n",
    "- ROC curves are useful for **comparing different classifiers**, since they take into account all possible thresholds.\n",
    " - The choice of a threshold **depends on the importance of TPR and FPR** classification problem.\n",
    " - If there is no external concern about low TPR or high FPR, one option is to weight them equally by choosing the threshold that maximizes 𝑇𝑃𝑅−𝐹𝑃𝑅, which the point closest to the **top left corner** of the ROC space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quadratic Discriminant Analysis\n",
    "\n",
    "**Quadratic discriminant analysis (QDA)** classifier results from assuming that the observations from each class are drawn from a Gaussian distribution, and plugging estimates for the parameters into Bayes’ theorem in order to perform prediction. \n",
    "\n",
    "However, unlike LDA, **QDA assumes that each class has its own covariance matrix**. That is, it assumes that an observation from the kth class is of the form $X ∼ N(μ_k,Σ_k)$, where $Σ_k$ is a covariance matrix for the kth class. Under this assumption, the Bayes classifier assigns an observation $X = x$ to the class for which\n",
    "\n",
    "\\begin{align}\n",
    "\\delta_k(x)&=-\\frac{1}{2}(x-\\mu_k)^TΣ_k^{-1}(x-\\mu_k)-\\frac{1}{2}\\log{|Σ_k|}+\\log{\\pi_k} \\\\\n",
    "&=-\\frac{1}{2}x^TΣ_k^{-1}x+x^TΣ_k^{-1}\\mu_k-\\frac{1}{2}\\mu_k^TΣ_k^{-1}\\mu_k-\\frac{1}{2}\\log{|Σ_k|}+\\log{\\pi_k}\n",
    "\\end{align} \n",
    "\n",
    "\n",
    "is largest.\n",
    "\n",
    "So the QDA classifier involves plugging estimates for $Σ_k, μ_k, π_k$ into $\\delta_k(x)$, and then assigning an observation $X = x$ to the class for which this quantity is largest. Unlike LDA, the quantity $x$ appears as a quadratic function.\n",
    "\n",
    "## Why does it matter whether or not we assume that the K classes share a common covariance matrix?\n",
    "\n",
    "The answer lies in the **bias-variance trade-off**:\n",
    " - When there are p predictors, then estimating a covariance matrix requires estimating p(p+1)/2 parameters. QDA estimates a separate covariance matrix for each class, for a total of Kp(p+1)/2 parameters.\n",
    " - Consequently, LDA is a much less flexible classifier than QDA, and so has substantially lower variance. \n",
    " - But there is a trade-off: if LDA’s assumption that the K classes share a common covariance matrix is badly off, then LDA can suffer from high bias. \n",
    "\n",
    "**Conclusion**\n",
    "\n",
    "- LDA tends to be a better bet than QDA if there are relatively few training observations and so reducing variance is crucial. \n",
    "- QDA is recommended if the training set is very large, so that the variance of the classifier is not a major concern, or if the assumption of a common covariance matrix for the K classes is clearly untenable\n",
    "\n",
    "<img src=\"./images/61.png\" width=700>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "255px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
