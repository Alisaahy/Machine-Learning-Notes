{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to represent text data\n",
    "\n",
    "Text is often referred to as “unstructured” data. This refers to the fact that text does not have the sort of structure that we normally expect for data: tables of records with fields having fixed meanings (essentially, collections of feature vectors), as well as links between the tables.\n",
    "\n",
    "\n",
    "A **document** is one piece of text, no matter how large or small. A document is composed of individual **tokens** or **terms**. A collection of documents is called a **corpus**.\n",
    "\n",
    "\n",
    "There're many ways to represent text data, including Bag-of-Words, TFIDF, etc. **We’ll typically experiment with different representations to see which produces the best results.**\n",
    "\n",
    "\n",
    "## Bag of Words\n",
    "\n",
    "In this model, a document is represented as the bag / collection of its individual words, **disregarding grammar, word order, sentence structure, and (usually) punctuation** but keeping **multiplicity**. Ittreats every word in a document as an **independent potential keyword (feature)** of the document, then assigns values to each document based on frequency and rarity.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Term Counts within Document: Term Frequency\n",
    "\n",
    "After transforming the document into a \"bag of words\", we can use various measures to characterize the document. The most common type of features calculated from the Bag-of-words model is **term frequency**, namely, the number of times a term appears in the text.\n",
    "\n",
    "\n",
    "**Perform some steps before create term frequency representation**\n",
    "\n",
    "1. First, the case has been **normalized**: every term is in lowercase.\n",
    "\n",
    "\n",
    "2. Many words have been **stemmed**: remove suffixes, transforms noun plurals to the singular forms.\n",
    "\n",
    "\n",
    "3. **Stopwords** have been removed. A stopword is a very common word in English (or whatever language is being parsed). The words the, and, of, and on are considered stopwords in English so they are typically removed.\n",
    "\n",
    "\n",
    "4. Some systems perform a step of normalizing the term frequencies with respect to **document length**.\n",
    "\n",
    "\n",
    "## Sparseness across Documents: Inverse Document Frequency\n",
    "\n",
    "So term frequency measures how prevalent a term is in a single document. We also have two opposing considerations:\n",
    "\n",
    "1. A term should not be too rare. For example, say the unusual word *prehensile* occurs in only one document in your corpus. This word will hardly be the basis of a meaningful document. For this reason, text processing systems usually impose a **lower limit** on the number of documents in which a term must occur.\n",
    "\n",
    "2. A term should not be too common. A term occurring in every document isn’t useful for classification because it doesn’t distinguish anything. Overly common terms are typically eliminated. One way to do this is to impose an arbitrary **upper limit** on the number (or fraction) of documents in which a word may occur.\n",
    "\n",
    "3. In addition to imposing upper and lower limits on term frequency, many systems consider the **distribution of the term over a corpus**. The **fewer documents** in which a term occurs, the **more significant** this term is likely to be in these documents it occurs in. \n",
    "\n",
    "This **sparseness of a term t** is measured commonly by an equation called **inverse document frequency (IDF)**.\n",
    "\n",
    "\\begin{align}\n",
    "IDF(t) = 1 + \\log(\\frac{\\text{total number of documents}}{\\text{number of documents containing t}})\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"./images/127.png\" width=600>\n",
    "\n",
    "- When a term is very rare (far left) the IDF is quite high. \n",
    "- It decreases quickly as the term becomes more common in documents, and approaches 1.0. \n",
    "- Most stopwords, due to their prevalence, will have an IDF near one.\n",
    "\n",
    "\n",
    "## Combining Term Frequency and Inverse Document Frequency: TF-IDF\n",
    "\n",
    "A very popular representation for text is the product of Term Frequency (TF) and Inverse Document Frequency (IDF), commonly referred to as **TFIDF**. The TFIDF value of a term t in a given document d is thus:\n",
    "\n",
    "\\begin{align}\n",
    "TFIDF(t,d) = TF(t,d) * IDF(t)\n",
    "\\end{align}\n",
    "\n",
    "Note that the **TFIDF value is specific to a single document (d)** whereas IDF depends on the entire corpus. Term counts **within the documents** form the TF values for each term, and the **document counts across the corpus** form the IDF values.\n",
    "\n",
    "**Each document thus becomes a feature vector, and the corpus is the set of these feature vectors.**\n",
    "\n",
    "\n",
    "**Feature Selection of Terms within Documents:**\n",
    "\n",
    "- Imposing minimum and maximum **thresholds** of term counts\n",
    "\n",
    "- Using a measure such as **information gain** to **rank the terms by importance** so that low-gain terms can be culled\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-gram Sequences\n",
    "\n",
    "Tthe bag-of-words representation treats every individual word as a term, discarding word order entirely. In some cases, **word order** is important and you want to preserve some information about it in the representation.\n",
    "\n",
    "\n",
    "**N-gram** include **sequences of adjacent words** as terms. For example, we could include pairs of adjacent words so that if a document contained the sentence “The quick brown fox jumps.” it would be transformed into the set of its constitutent words {quick, brown, fox, jumps}, plus the tokens **quick_brown, brown_fox, and fox_jumps**.\n",
    "\n",
    "\n",
    "**bag of n-grams up to three**: representing each document using its individual words, adjacent word pairs, and adjacent word triples as features.\n",
    "\n",
    "\n",
    "\n",
    "**Advantages**:\n",
    "\n",
    "- N-grams are useful when particular phrases are significant but their component words may not be.\n",
    "\n",
    "- They are easy to generate; they require no linguistic knowledge or complex parsing algorithm.\n",
    "\n",
    "\n",
    "**Disadvantages**:\n",
    "\n",
    "- Greatly increase the size of the feature set. We may have to do feature selection or give special consideration to computational storage space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named Entity Extraction\n",
    "\n",
    "Recognize common named entities in documents, like *Silicon Valley, New York Mets, Department of the Interior, and Game of Thrones*. Their com‐ ponent words mean one thing, and may not be significant, but in sequence they name unique entities with interesting identities.\n",
    "\n",
    "With different extractors, the quality of entity recognition can vary. Some extractors may have particular areas of expertise, such as industry, government, or popular culture. **SpaCy** is one kind of library for name entity extraction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Models\n",
    "\n",
    "Because of the complexity of language and documents, sometimes we want an **additional layer** between the document and the model. In the context of text we call this the topic layer.\n",
    "\n",
    "- As before, each document constitutes a sequence of words, but instead of the words being used directly by the final classifier, the words map to one or more **topics**.\n",
    "\n",
    "- The topics also are learned from the data (often via **unsupervised data mining**). We can think of the topic layer as being a **clustering of words**. As with clusters, the topics emerge from statistical regularities in the data. General methods for creating topic models include matrix factorization methods, such as Latent Semantic Indexing and Probabilistic Topic Models, such as Latent Dirichlet Allocation. \n",
    "\n",
    "- The final classifier is defined in terms of these **intermediate topics** rather than words. \n",
    "\n",
    "\n",
    "<img src=\"./images/128.png\" width=600>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "286px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
