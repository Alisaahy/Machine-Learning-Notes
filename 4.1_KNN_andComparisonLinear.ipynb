{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# k-Nearest Neighbors\n",
    "\n",
    "Given a positive integer K and a test observation $x_0$, the KNN regression first identifies the $K$ points in the training data  that are closest to $x_0$, represented by $N_0$. It then estimates $f(x_0)$ using the probability estimation of a given class j among all the training responses in $N_0$:\n",
    "\n",
    "\\begin{align}\n",
    "\\hat{f}(x_0)=\\frac{1}{K}\\sum_{i\\in N_0}I(y_i = j)\n",
    "\\end{align}\n",
    "\n",
    "Suppose we choose K = 3. Then KNN will identify the three observations that are closest to the point we want to predict.\n",
    "Assume among all the neighbours, 2 are bluepoints and 1 orange, resulting in estimated $f(x_0)$ 2/3 for blue class ad 1/3 for the orange class. Hence KNN will predict the point to the blue class.\n",
    "<img src=\"images/3.png\" width=\"200\">\n",
    "\n",
    "- The optimal value for K will depend on the **bias-variance trade-off**.\n",
    " - A smaller value of K provides a more flexible fit, which will \n",
    " have low bias but high variance. This variance is due to the fact that \n",
    " the prediction in a given region is entirely dependent on just one observation.\n",
    " - A largee values of K provide a smoother and less variable fit, which will \n",
    " have high bias but low variance. \n",
    " The prediction in a region is given by the overall probability in a region, \n",
    " and so changing one observation has a smaller effect. \n",
    " However, the smoothing may cause bias by masking some of the structure in f(X).\n",
    " - If K decreases K, in the regression setting, \n",
    " the training error rate consistently declines as the flexibility increases, \n",
    " but the test error exhibits a U-shape, declining at first and increasing again when the model becomes excessively flexible and overfits.\n",
    " \n",
    "<img src=\"images/4.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison of Linear Regression with KNN\n",
    "\n",
    "**The parametric approach will outperform the nonparametric approach if the parametric form that has been selected is close to the true form of $f$, because non-parametric approach incurs a cost in variance that is not offset by a reduction in bias.**\n",
    "\n",
    "<img src=\"images/37.png\" width=\"500\">\n",
    "\n",
    "- KNN performs slightly worse than linear regression when the relationship is linear, but much better than linear regression for non-linear situations.\n",
    "\n",
    "<img src=\"images/36.png\" width=\"500\">\n",
    "\n",
    "\n",
    "**For multi-dimensional data, the increase in dimension has only caused a small deterioration in the linear regression test set MSE, but it has caused more than a ten-fold increase in the MSE for KNN.**\n",
    "\n",
    "This decrease in performace as the dimension increases is a common problem for KNN, and results from the fact that in higher dimensions tehre's effectively a reduction in sample size. For example, in this dataset, there're 1-- training observations; where p=1, this provides enough information to accurately estimate $f$. However, spreading 100 observations over p=20 dimensions results in a phenomenon in which a given observation has no **nearby neighbors** - this is the so-calld **curved of dimensionality** That is, the K observations that are nearest to a given test observation ${x_0}$ in p-dimensional space when p is large, leading to very poor prediction of $f$ and hence a poor KNN fit.\n",
    "\n",
    "**As a general rule, parametric methods will tend to outperform non-parametric approaches when there's a small number of observations per predictor.**\n",
    "\n",
    "**Even in problems in which the dimension is small, we might prefer linear regression to KNN from an interpretability standpoint. If the test MSE of KNN is only slightly lower than that of linear regression, we might be willing to forego a little bit of prediction accuracy for the sake of a simple model that can be described in terms of just a few coefficients, and for which p-values are available.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "kNN: k Nearest Neighbors\n",
    "\n",
    "Input:      X_test: input array to classify (1xN)\n",
    "            X_train: training dataset of known vectors with size M (NxM)\n",
    "            y_train: target labels for training dataset (1xM vector)\n",
    "            k: number of neighbors to use \n",
    "\n",
    "Output:     y_pred: predicted class label for X_test\n",
    "'''\n",
    "\n",
    "import numpy as np\n",
    "import operator\n",
    "\n",
    "def knn(X_train, y_train, X_test, k):\n",
    "    # Get number of rows in the whole X_train dataset\n",
    "    train_size = X_train.shape[0]\n",
    "    \n",
    "    # numpy.tile(arr, repetitions) : constructs a new array by repeating array the number of times\n",
    "    # Tile the input vector to be the same shape as the training set \n",
    "    # Get the difference between elements in each observation and elements in the input vector\n",
    "    diffMat = np.tile(X_test, (train_size, 1)) - X_train\n",
    "    \n",
    "    # Calculate the Euclidian distance between the each observation in the training dataset and the input vector\n",
    "    sqDiffMat = diffMat ** 2\n",
    "    \n",
    "    # After .sum, sqdistances becomes a 1xM vector.\n",
    "    sqdistances = sqDiffMat.sum(axis=1)\n",
    "    \n",
    "    # Distance is also a 1xM vector. Each element is the observation's distance between the input vector.\n",
    "    distances = sqdistances ** 0.5\n",
    "    \n",
    "    # argsort: returns the indices that would sort an array.\n",
    "    # sortedDistIndicies[0] is the index of the smallest value in the distances vector\n",
    "    sortedDistIndicies = distances.argsort()\n",
    "    \n",
    "    # Create a dictionary to count the number of elements in each class among the first k data\n",
    "    classCount = {}\n",
    "    \n",
    "    for i in range(k):\n",
    "        # Extract the label of the k nearest neighbors\n",
    "        labelType = y_train[sortedDistIndicies[i]]\n",
    "        classCount[labelType] = classCount.get(labelType, 0) + 1\n",
    "    \n",
    "    # .items decompose the dictionary into a list of tuples: [(labelType1, number), (labelType2, number)]\n",
    "    # operator.itemgetter returns numbers in each (labelType1, number) tuple\n",
    "    sortedClassCount = sorted(classCount.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    \n",
    "    # Return the labelType with the most elements among the k nearest neighbors\n",
    "    y_pred = sortedClassCount[0][0]\n",
    "    print('The predicted class for test data is: ', y_pred)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted class for test data is:  a\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'a'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test case\n",
    "X_train = np.array([[1, 2, 3, 4, 5],\n",
    "                    [3, 4, 5, 6, 7],\n",
    "                    [4, 5, 6, 7, 8],\n",
    "                    [5, 6, 7, 8, 9],\n",
    "                    [6, 7, 8, 9, 10]])\n",
    "y_train = np.array(['a', 'b', 'a', 'b', 'a'])\n",
    "X_test = np.array([4, 3, 4, 3, 6])\n",
    "k = 3\n",
    "\n",
    "knn(X_train, y_train, X_test, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
