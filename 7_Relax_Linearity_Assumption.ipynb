{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact, the linearity assumption is almost always an approximation, and sometimes a poor one. Sometimes, we need to relax the linearity assumption while still attempting to maintain as much interpretability as possible.\n",
    "\n",
    "# Polynomial Regression\n",
    "\n",
    "To represent the non-linear relationship between predictors and the response, we can use **polynomial regression**. In general, we can model the expected value of 'y' as some higher degree polynomial, yielding the general polynomial regression model\n",
    "\n",
    "\\begin{align}\n",
    "y = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\beta_3 x^3 + \\cdots + \\beta_n x^n + \\varepsilon. \\, \n",
    "\\end{align}   \n",
    "\n",
    "\n",
    "The coefficients can be easily estimated using **least squares linear regression** because this is just a standard linear model with predictors of higher degree polynomial.\n",
    "\n",
    "**Attention**\n",
    "\n",
    "\n",
    "Generally speaking, it is unusual to use polynomials of degree higher than 3 or 4, because in this case, the polynomial curve can become **overly flexible** and can take on some very strange shapes. This is especially true near the boundary of the X variable.\n",
    "\n",
    "**Logistic regression**\n",
    "\n",
    "We can also use polynomial functions to build logisic regression models and predict binary responses:\n",
    "\n",
    "<img src=\"./images/76.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step Functions / Piecewise-constant regression\n",
    "\n",
    "Break the range of X into bins, and fit a different constant in each bin and avoid imposing a global structure. \n",
    "\n",
    "**Steps**\n",
    "1. Create cutpoints c1, c2, . . . , cK in the range of X, and then construct K + 1 new **dummy variables** that returns a 1 if the condition is true, and returns a 0 otherwise.\n",
    "<img src=\"./images/77.png\" width=\"300\">\n",
    "\n",
    "\n",
    "2. Use least squares to fit a linear model using C1(X), C2(X), . . . , CK (X) as predictors.\n",
    "\n",
    "\\begin{align}\n",
    "y = \\beta_0 + \\beta_1C_1(x_i) + \\beta_2C_2(x_i) + \\beta_3C_3(x_i) + \\cdots + \\beta_KC_K(x_i) + \\varepsilon_i. \n",
    "\\end{align}   \n",
    "and βj represents the average increase in the response for X in cj ≤ X < cj+1 relative to X < c1\n",
    "\n",
    "\n",
    "3. Unless there are natural breakpoints in the predictors, piecewise-constant functions can miss the trend."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basis Functions\n",
    "\n",
    "Polynomial and piecewise-constant regression models are in fact special cases of a basis function approach. \n",
    "\n",
    "**Basis functions**\n",
    "\n",
    "Instead of fitting a linear model in X, we fit the model, which is a standard linear model with predictors b1(xi),b2(xi),...,bK(xi). Hence, we can use least squares to estimate the unknown regression coefficients.\n",
    "\n",
    "\\begin{align}\n",
    "y = \\beta_0 + \\beta_1b_1(x_i) + \\beta_2b_2(x_i) + \\beta_3b_3(x_i) + \\cdots + \\beta_Kb_K(x_i) + \\varepsilon_i. \n",
    "\\end{align}\n",
    "\n",
    "\n",
    "The basis functions b1(·),b2(·),...,bK(·) are fixed and known.  For polynomial regression, the basis functions are $b_j(x_i) = x_j^i$, and for piecewise constant functions they are $b_j(x_i) = I(cj ≤ xi < cj+1)$.\n",
    "\n",
    "Many alternatives of basis functions are possible: wavelets, Fourier series, regression splines, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression Splines\n",
    "\n",
    "We create regression splines by specifying **a set of knots**, producing a sequence of **basis functions**, and then using **least squares** to estimate the spline coefficients. \n",
    "\n",
    "**Drawbacks**\n",
    "\n",
    "If we don’t put any constraints on the function, then we can always make RSS zero simply by choosing a function that interpolate all of the reponses y. Such a function would woefully **overfit** the data—it would be far too flexible.\n",
    "\n",
    "\n",
    "## Piecewise Polynomials\n",
    "\n",
    "Instead of fitting a high-degree polynomial over the entire range of X, piece-wise polynomial regression involves **fitting separate low-degree polynomials over different regions of X**\n",
    "\n",
    "e.g. A piecewise cubic polynomial with a single knot at a point c takes the form:\n",
    "\n",
    "<img src=\"./images/78.png\" width=\"450\">\n",
    "\n",
    "In other words, we fit two different polynomial functions to the data, one on the subset of the observations with xi < c, and one on the subset of the observations with xi ≥ c. Each of these polynomial functions can be fit using **least squares** applied to simple functions of the original predictor. Since each polynomial has four parameters, we are using a total of **degrees of freedom** in fitting this piecewise polynomial model.\n",
    "\n",
    "Using **more knots** leads to a **more flexible** piecewise polynomial. In general, if we place K different knots throughout the range of X, then we will end up fitting K + 1 different cubic polynomials. \n",
    "\n",
    "\n",
    "## Constraints and Splines\n",
    "\n",
    "<img src=\"./images/82.png\" width=\"600\">\n",
    "\n",
    "Each constraint that we impose on the piecewise cubic polynomials effectively frees up one degree of freedom, by reducing the complexity of the resulting piecewise polynomial fit.\n",
    "\n",
    "So in the top left plot, we are using 8 degrees of freedom, but in the bottom left plot we imposed 3 constraints (continuity, continuity of the first derivative, and continuity of the second derivative) and so are left with 5 degrees of freedom.\n",
    "\n",
    "**In general, a cubic spline with K knots uses a total of 4+K degrees of freedom**\n",
    "\n",
    "\n",
    "**Natural Spline**\n",
    "\n",
    "A natural spline is a regression spline with additional boundary constraints: the function is required to be linear at the boundary (in the region where X is smaller than the smallest knot, or larger than the largest knot). This additional constraint means that natural splines generally produce more stable estimates at the boundaries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Smoothing Splines\n",
    "\n",
    "To avoid overfitting in regression spline, what we really want is a function g that makes RSS small, but also that is also smooth. \n",
    "\n",
    "**How can we ensure that g is smooth?** We can find the function g that minimizes:\n",
    "\n",
    "\\begin{align}\n",
    "y = \\sum_{i=1}^n (y_i-g(x_i))^2 + \\lambda\\int g''(t)^2dt\n",
    "\\end{align}  \n",
    "\n",
    "\n",
    "- The term $\\sum_{i=1}^n (y_i-g(x_i))^2$ is a **loss function** that encourages g to fit the data well.\n",
    "\n",
    "- The term $\\lambda\\int g''(t)^2dt$ is a **penalty term** that penalizes the variability in g. It measures the **total change in the first derivative of the function g**, over its entire range. If g is very smooth, then g′(t) will be close to constant and 􏰟 $g''(t)^2dt$ will take on a small value.\n",
    " - The notation $g''(t)^2dt$ indicates the second derivative of the function g. It measures **the amount by which the slope of a function is changing**, from which we can know the function's roughness. \n",
    " - The $\\int$ is an integral, which we can think of as a summation over the range of t.\n",
    " \n",
    "- When λ = 0, then the penalty term has no effect, and so the function g will be very **jumpy** and will exactly interpolate the training observations. When λ → ∞, g will be perfectly smooth—it will just be a **linear least squares straight line**. This parameter can be chosen by **cross-validation**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local Regression\n",
    "\n",
    "Local regression fit flexible non-linear functions, which involves **computing the fit at a target point x0 using only the *nearby* training observations**.\n",
    "\n",
    "<img src=\"./images/79.png\" width=\"450\">\n",
    "\n",
    "<img src=\"./images/80.png\" width=\"600\">\n",
    "\n",
    "\n",
    "**Span s**\n",
    "\n",
    "The span s plays a role like that of the tuning parameter λ in smoothing splines: it controls the flexibility of the non-linear fit. \n",
    "- The smaller the value of s, the more local and wiggly will be our fit; \n",
    "- Alternatively, a very large value of s will lead to a global fit to the data using all of the training observations.\n",
    "- We can again use cross-validation to choose s\n",
    "\n",
    "<img src=\"./images/81.png\" width=\"600\">\n",
    "\n",
    "\n",
    "**Higher dimensional data**\n",
    "\n",
    "- For 2-dimensional data, we can simply use two-dimensional neighborhoods, and fit bivariate linear regression models using the observations that are near each target point in two-dimensional space. \n",
    "- **Theoretically** the same approach can be imple- mented in higher dimensions, using linear regressions fit to p-dimensional neighborhoods. However, local regression can perform poorly if **p is much larger than about 3 or 4** because there will generally be very few training observations close to x0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generalized Additive Models\n",
    "\n",
    "**Generalized additive models** (GAMs) provide a general framework for extending a standard linear model by allowing non-linear functions of each ofthevariables,while maintaining **additivity**.\n",
    "\n",
    "\n",
    "<img src=\"./images/83.png\" width=\"450\">\n",
    "This is an example of a GAM.\n",
    "\n",
    "<img src=\"./images/84.png\" width=\"450\">\n",
    "<img src=\"./images/85.png\" width=\"450\">\n",
    "\n",
    "In most situations, the differences in the GAMs obtained using smoothing splines versus natural splines are small. Fitting a GAM with a smoothing spline is not quite as simple as fitting a GAM with a natural spline, since in the case of smoothing splines, least squares cannot be used. \n",
    "\n",
    "However, standard software such as the gam() function in R can be used to fit GAMs using smoothing splines, via an approach known as **backfitting**. \n",
    "\n",
    "\n",
    "## Pros and Cons of GAMs\n",
    "\n",
    "**PROS**\n",
    "1. GAMs allow us to fit a non-linear fj to each Xj, so that we can automatically model non-linear relationships.\n",
    "2. The non-linear fits can potentially make more accurate predictions for the response Y .\n",
    "3. Because the model is additive, we can still examine the effect of each Xj on Y individually while holding all of the other variables fixed. Hence if we are interested in **inference**, GAMs provide a useful representation.\n",
    "4. The smoothness of the function fj for the variable Xj can be sum- marized via degrees of freedom.\n",
    "\n",
    "**CONS**\n",
    "The main limitation of GAMs is that the model is restricted to be additive. With many variables, important interactions can be missed. In addition we can add low-dimensional interaction functions of the form $f_{jk}(X_j,X_k)$ into the model; such terms can be fit using two-dimensional smoothers such as local regression, or two-dimensional splines (not covered here).\n",
    "\n",
    "\n",
    "## GAMs for Classification Problems\n",
    "\n",
    "\n",
    "This logit is the log of the odds of P(Y = 1|X) versus P(Y = 0|X), which represents as a linear function of the predictors.\n",
    "\n",
    "<img src=\"./images/86.png\" width=\"450\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
